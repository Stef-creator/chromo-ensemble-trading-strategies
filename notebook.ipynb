{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37997b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# utility functions\n",
    "from utils import *\n",
    "\n",
    "# Models\n",
    "from models.chromosome_model import create_ga_chromosome_metrics\n",
    "from models.cnn_model import *\n",
    "from models.lstm_model import *\n",
    "from models.transformer_model import *\n",
    "from models.mlp_model import *\n",
    "\n",
    "# Scripts\n",
    "from scripts.train_ga import *\n",
    "from scripts.train_mlp import *\n",
    "from scripts.train_cnn import *\n",
    "from scripts.train_lstm import *\n",
    "from scripts.train_transformer import *\n",
    "from scripts.ensemble import *\n",
    "\n",
    "# Set seeds\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Ensure deterministic behavior in PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set environment seed (optional)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc8fcb",
   "metadata": {},
   "source": [
    "# Ensemble Trading Strategy Pipeline\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Data Preparation](#Data-Preparation)\n",
    "3. [Genetic Algorithm Best Chromosome](#Genetic-Algorithm-Best-Chromosome)\n",
    "4. [Model Loading and Predictions](#Model-Loading-and-Predictions)\n",
    "   - 4.1 [MLP Model](#MLP-Model)\n",
    "   - 4.2 [CNN Model](#CNN-Model)\n",
    "   - 4.3 [LSTM Model](#LSTM-Model)\n",
    "   - 4.4 [Transformer Model](#Transformer-Model)\n",
    "5. [Ensemble Voting Strategies](#Ensemble-Voting-Strategies)\n",
    "   - 5.1 [Majority Voting](#Majority-Voting)\n",
    "   - 5.2 [Probabilistic Voting](#Probabilistic-Voting)\n",
    "6. [Backtesting and Evaluation](#Backtesting-and-Evaluation)\n",
    "   - 6.1 [Performance Metrics](#Performance-Metrics)\n",
    "   - 6.2 [Advanced Trading Metrics](#Advanced-Trading-Metrics)\n",
    "7. [Visualization](#Visualization)\n",
    "   - 7.1 [Equity Curve](#Equity-Curve)\n",
    "   - 7.2 [Prediction Signals](#Prediction-Signals)\n",
    "   - 7.3 [Confusion Matrix](#Confusion-Matrix)\n",
    "8. [Conclusions and Next Steps](#Conclusions-and-Next-Steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ebedd",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates a comprehensive ensemble trading strategy pipeline integrating:\n",
    "\n",
    "- Genetic Algorithm optimization for technical indicator parameters\n",
    "- Machine learning models including MLP, CNN, LSTM, and Transformer architectures\n",
    "- Ensemble voting methods, both majority and probabilistic\n",
    "- Advanced backtesting and evaluation metrics for trading performance\n",
    "\n",
    "The goal of this pipeline is to combine multiple predictive models and optimized technical signals to improve trading decision robustness, profitability, and risk-adjusted returns. The models are trained and tuned on historical financial data, and their outputs are evaluated through systematic backtesting with both static and dynamic position sizing strategies.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "- Genetic Algorithm for RSI parameter tuning  \n",
    "- MLP, CNN, LSTM, and Transformer models with hyperparameter tuning  \n",
    "- Majority voting and probabilistic ensemble integration  \n",
    "- Backtesting with advanced trading performance metrics (Sharpe, Sortino, Calmar, Omega, Gain to Pain, etc.)  \n",
    "- Modular, reproducible pipeline for systematic trading research\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210de2e",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In this section, we load historical financial data for the selected ticker, apply technical indicators, and prepare the dataset for model input.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Download historical price data using the Yahoo Finance API wrapper.\n",
    "2. Calculate technical indicators:\n",
    "   - Relative Strength Index (RSI) over multiple intervals\n",
    "   - Simple Moving Averages (SMA) for trend identification\n",
    "3. Define trend labels based on SMA crossovers to classify uptrends and downtrends.\n",
    "\n",
    "The prepared dataset will be used for generating features and labels required by each machine learning model in the pipeline. Here it is possible to change the ticker to another of your choosing, in this notebook we will be using the Tesla stock ticker.\n",
    "\n",
    "Other examples could be:\n",
    "- ticker = \"BTC-USD\"  # Bitcoin\n",
    "- ticker = \"GLD\"  # SPDR Gold Shares\n",
    "- ticker = \"XLF\"  # Financial sector ETF\n",
    "- ticker = \"TSLA\"  # Tesla, for a high-volatility equity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0484263",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_stock_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m start_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1997-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m end_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2017-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_stock_data\u001b[49m(ticker, start_date, end_date)\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m add_technical_indicators(df, ticker)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'download_stock_data' is not defined"
     ]
    }
   ],
   "source": [
    "ticker = \"TSLA\"\n",
    "start_date = \"1997-01-01\"\n",
    "end_date = \"2017-01-01\"\n",
    "df = download_stock_data(ticker, start_date, end_date)\n",
    "df = add_technical_indicators(df, ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94015279",
   "metadata": {},
   "source": [
    "## Genetic Algorithm Best Chromosome\n",
    "\n",
    "In this section, we load the optimized RSI and interval parameters generated by the Genetic Algorithm (GA) tuning process.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The GA searches for the combination of RSI thresholds and interval lengths that maximize a predefined *fitness function* related to trading performance. These optimized parameters are critical for generating features and labels for model training and evaluation.\n",
    "\n",
    "### Optimization Problem\n",
    "\n",
    "The GA seeks to maximize the *fitness function* defined as:\n",
    "\n",
    "$$\n",
    "\\text{Fitness} = \\frac{R_a}{|\\text{Max Drawdown}|}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ R_a $ is the *annualized return*, calculated as:\n",
    "\n",
    "$$\n",
    "R_a = \\left( \\frac{P_{end}}{P_{start}} \\right)^{\\frac{252}{N}} - 1\n",
    "$$\n",
    "\n",
    "Here:\n",
    "  - $ P_{end} $ = portfolio value at end\n",
    "  - $ P_{start} $ = portfolio value at start\n",
    "  - $ N $ = number of trading days\n",
    "\n",
    "- *Max Drawdown* is the largest observed loss from a peak to a trough before a new peak is reached, calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Max Drawdown} = \\min_t \\left( \\frac{P_t - \\max_{i \\leq t} P_i}{\\max_{i \\leq t} P_i} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Chromosome Representation\n",
    "\n",
    "Each *chromosome* in the GA population encodes a set of RSI-based trading parameters:\n",
    "\n",
    "- Downtrend buy value and interval\n",
    "- Downtrend sell value and interval\n",
    "- Uptrend buy value and interval\n",
    "- Uptrend sell value and interval\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. *Load GA results* from the saved CSV file containing the final tuned chromosomes.\n",
    "2. *Extract the best chromosome* based on the highest fitness score.\n",
    "3. *Prepare these optimized parameters* for feature generation and labeling in the machine learning pipeline.\n",
    "\n",
    "These parameters ensure that models are trained using historically optimal RSI thresholds and intervals, enhancing trading signal relevance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ga_chromosome_metrics(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55cce5",
   "metadata": {},
   "source": [
    "### Genetic Algorithm Chromosome Performance Interpretation\n",
    "\n",
    "The table below summarizes the performance of multiple chromosomes, each representing a set of RSI thresholds and intervals optimized by the Genetic Algorithm (GA).\n",
    "\n",
    "| Metric | Explanation |\n",
    "|--------|-------------|\n",
    "| down_buy_val / down_buy_int | RSI buy threshold and interval during downtrend |\n",
    "| down_sell_val / down_sell_int | RSI sell threshold and interval during downtrend |\n",
    "| up_buy_val / up_buy_int | RSI buy threshold and interval during uptrend |\n",
    "| up_sell_val / up_sell_int | RSI sell threshold and interval during uptrend |\n",
    "| total_return | Overall return factor (e.g. 1.12 = +12%) |\n",
    "| annualized_return | Annualized return scaled to a yearly basis |\n",
    "| sharpe_ratio | Risk-adjusted return accounting for standard deviation |\n",
    "| sortino_ratio | Risk-adjusted return penalizing only downside volatility |\n",
    "| max_drawdown | Largest peak-to-trough decline observed |\n",
    "| fitness | Defined as annualized return divided by absolute max drawdown |\n",
    "\n",
    "#### Key insights\n",
    "\n",
    "- Highest fitness:  \n",
    "  - Chromosome index 4 with fitness = 0.8909, annualized return = 14.5%, and very low max drawdown = –16.3%.  \n",
    "  - This represents the best risk-adjusted performance under the defined fitness metric.\n",
    "\n",
    "- Highest total return:  \n",
    "  - Chromosome index 2 with total return = 3.38x (238% net gain), annualized return = 25.5%, and max drawdown = –40.6%.  \n",
    "  - While it achieves the highest returns, the drawdown is significantly higher, reducing its fitness relative to index 4.\n",
    "\n",
    "#### Strategic takeaway\n",
    "\n",
    "The optimal chromosome selection depends on trading objectives:\n",
    "\n",
    "- If prioritizing maximum absolute returns, chromosome index 2 is preferable despite higher drawdown.\n",
    "- If prioritizing risk-adjusted returns with controlled drawdown, chromosome index 4 is optimal under the GA fitness definition.\n",
    "\n",
    "These optimized parameters will guide feature generation and labeling for all subsequent machine learning models in the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda66172",
   "metadata": {},
   "source": [
    "## Alternative Fitness Functions for Genetic Algorithm Optimization\n",
    "\n",
    "Below are alternative fitness functions that have been implemented in this code that be used to optimize trading strategy performance, each with its mathematical definition and interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "### Sharpe Ratio\n",
    "\n",
    "Measures *risk-adjusted return*, penalizing both upside and downside volatility.\n",
    "\n",
    "$$\n",
    "\\text{Sharpe Ratio} = \\frac{R_p - R_f}{\\sigma_p}\n",
    "$$\n",
    "\n",
    "- $ R_p $: portfolio return  \n",
    "- $ R_f $: risk-free rate  \n",
    "- $ \\sigma_p $: standard deviation of portfolio returns\n",
    "\n",
    "### Sortino Ratio\n",
    "\n",
    "Adjusts for *downside risk only*, ignoring upside volatility.\n",
    "\n",
    "$$\n",
    "\\text{Sortino Ratio} = \\frac{R_p - R_f}{\\sigma_D}\n",
    "$$\n",
    "\n",
    "- $ \\sigma_D $: standard deviation of negative returns (downside deviation)\n",
    "\n",
    "---\n",
    "\n",
    "### Strategic Note\n",
    "\n",
    "Selecting a fitness function depends on your *trading objectives* (max return, max risk-adjusted return, drawdown minimization) and your *risk tolerance* for the strategy.\n",
    "\n",
    "Consider testing multiple objectives to identify which yields the most robust out-of-sample performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238cff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "genetic_algorithm(ticker, override_selection_metric='sharpe_ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce926d6",
   "metadata": {},
   "source": [
    "## Model Loading and Predictions\n",
    "\n",
    "In this section, we load each trained machine learning model and generate their predictions on the prepared labeled dataset.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The ensemble pipeline integrates predictions from multiple models to enhance trading signal robustness. Each model was trained using the optimized technical indicator parameters obtained from the Genetic Algorithm and tuned for hyperparameter configurations.\n",
    "\n",
    "### Models included:\n",
    "\n",
    "1. *MLP (Multi-Layer Perceptron)*\n",
    "2. *CNN (Convolutional Neural Network)*\n",
    "3. *LSTM (Long Short-Term Memory Network)*\n",
    "4. *Transformer Model*\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. *Load trained model weights* from saved `.pth` files.\n",
    "2. *Prepare model input data* with correct feature structure.\n",
    "3. *Generate predictions* using each model in evaluation mode.\n",
    "4. *Store predictions for ensemble integration.*\n",
    "\n",
    "The outputs from this section will be used to construct ensemble voting strategies in subsequent analyses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7df05",
   "metadata": {},
   "source": [
    "### MLP Model\n",
    "\n",
    "In this subsection, we load the trained Multi-Layer Perceptron (MLP) model and generate its predictions on the prepared labeled dataset.\n",
    "\n",
    "#### Model overview\n",
    "\n",
    "The *MLP model* is a fully connected feedforward neural network trained to classify trading actions (Buy, Sell, Hold) based on RSI features and trend information.\n",
    "\n",
    "#### Mathematical formulation\n",
    "\n",
    "An MLP with $ L $ layers can be formulated as:\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "& h^{(0)} = x \\\\\n",
    "& h^{(l)} = \\sigma(W^{(l)} h^{(l-1)} + b^{(l)}), \\quad l = 1, \\ldots, L-1 \\\\\n",
    "& \\hat{y} = \\text{softmax}(W^{(L)} h^{(L-1)} + b^{(L)})\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $ x $ is the input feature vector (e.g. RSI value, interval, trend).  \n",
    "- $ W^{(l)} $ and $ b^{(l)} $ are the weights and biases of layer $ l $.  \n",
    "- $ \\sigma $ is the activation function (e.g. ReLU).  \n",
    "- softmax output yields class probabilities for Buy, Sell, Hold.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Load model architecture with the tuned hidden layer configuration.\n",
    "2. Load trained weights from the saved `.pth` file.\n",
    "3. Prepare input tensor with required feature columns.\n",
    "4. Generate predictions and extract class labels for ensemble integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tuning_mlp(ticker)\n",
    "best_mlp(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ac505",
   "metadata": {},
   "source": [
    "### CNN Model\n",
    "\n",
    "In this subsection, we load the trained Convolutional Neural Network (CNN) model and generate its predictions on the prepared labeled dataset.\n",
    "\n",
    "#### Model overview\n",
    "\n",
    "The CNN model captures local temporal patterns in RSI and trend features by applying convolutional filters over input sequences, learning hierarchical representations relevant for trading signal classification.\n",
    "\n",
    "#### Mathematical formulation\n",
    "\n",
    "For a 1D CNN layer:\n",
    "\n",
    "$$\n",
    "h^{(l)}_i = \\sigma \\left( \\sum_{k=0}^{K-1} W_k^{(l)} x_{i+k} + b^{(l)} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ x $ is the input sequence.  \n",
    "- $ W_k^{(l)} $ are the convolutional kernel weights of size $ K $.  \n",
    "- $ b^{(l)} $ is the bias term.  \n",
    "- $ \\sigma $ is the activation function (e.g. ReLU).  \n",
    "- $ h^{(l)}_i $ is the output feature map at position $ i $ in layer $ l $.\n",
    "\n",
    "The final output is passed through fully connected layers and a softmax function to produce class probabilities for Buy, Sell, Hold.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Load model architecture with tuned kernel size and hidden channels.  \n",
    "2. Load trained weights from the saved `.pth` file.  \n",
    "3. Prepare input tensors as rolling window sequences.  \n",
    "4. Generate predictions and extract class labels for ensemble integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_cnn_hyperparameters(ticker)\n",
    "best_cnn(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a17496",
   "metadata": {},
   "source": [
    "### 4.3 LSTM Model\n",
    "\n",
    "In this subsection, we load the trained Long Short-Term Memory (LSTM) model and generate its predictions on the prepared labeled dataset.\n",
    "\n",
    "#### Model overview\n",
    "\n",
    "The LSTM model is a recurrent neural network architecture designed to capture sequential dependencies and temporal patterns in time series data, addressing the vanishing gradient problem present in traditional RNNs.\n",
    "\n",
    "#### Mathematical formulation\n",
    "\n",
    "An LSTM cell operates with the following equations at each time step $ t $:\n",
    "\n",
    "\\begin{aligned}\n",
    "& f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f) \\\\\n",
    "& i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i) \\\\\n",
    "& o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o) \\\\\n",
    "& \\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c) \\\\\n",
    "& c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\\\\n",
    "& h_t = o_t \\odot \\tanh(c_t)\n",
    "\\end{aligned}\n",
    "\n",
    "where:\n",
    "\n",
    "- $ x_t $ is the input vector at time $ t $.  \n",
    "- $ h_{t-1} $ is the previous hidden state.  \n",
    "- $ c_t $ is the cell state.  \n",
    "- $ f_t, i_t, o_t $ are the forget, input, and output gates, respectively.  \n",
    "- $ \\tilde{c}_t $ is the candidate cell state.  \n",
    "- $ W $ and $ U $ are weight matrices, $ b $ are biases.  \n",
    "- $ \\sigma $ denotes the sigmoid activation function, and $ \\odot $ denotes element-wise multiplication.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Load model architecture with tuned hidden size and number of layers.  \n",
    "2. Load trained weights from the saved `.pth` file.  \n",
    "3. Prepare input tensors as sequential rolling windows.  \n",
    "4. Generate predictions and extract class labels for ensemble integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0637e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_lstm_hyperparameters(ticker)\n",
    "best_lstm(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11055815",
   "metadata": {},
   "source": [
    "### Transformer Model\n",
    "\n",
    "In this subsection, we load the trained Transformer model and generate its predictions on the prepared labeled dataset.\n",
    "\n",
    "#### Model overview\n",
    "\n",
    "The Transformer model uses self-attention mechanisms to capture dependencies across the entire input sequence, enabling parallel computation and effective long-range pattern recognition in time series data.\n",
    "\n",
    "#### Mathematical formulation\n",
    "\n",
    "The Scaled Dot-Product Attention mechanism in the Transformer is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ Q $ = Query matrix  \n",
    "- $ K $ = Key matrix  \n",
    "- $ V $ = Value matrix  \n",
    "- $ d_k $ = dimension of the key vectors\n",
    "\n",
    "---\n",
    "\n",
    "The Multi-Head Attention combines multiple attention heads:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "where each head is computed as:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Load model architecture with tuned dimensions, number of heads, and layers.  \n",
    "2. Load trained weights from the saved `.pth` file.  \n",
    "3. Prepare input tensors as sequential feature windows.  \n",
    "4. Generate predictions and extract class labels for ensemble integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c302cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_transformer(ticker)\n",
    "best_transformer(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f558fab",
   "metadata": {},
   "source": [
    "### Ensemble Model Comparison and Evaluation\n",
    "\n",
    "In this section, we compare the performance of different *ensemble strategies* tested on our true out-of-sample dataset. Here, I am defining true out-of-sample dataset as the period of time from 2017-01-01 until today. This implies that the model has had 20 years to train on, however the model will be tested on only 8.5 years.\n",
    "\n",
    "Furthermore, rather than relying upon one model alone, focus exclusively on ensemble approaches that combine the predictive power of MLP, CNN, LSTM, and Transformer models. Therefore we will not be evaluating each model individually in this notebook. The ensemble approach is as follows:\n",
    "\n",
    "#### Ensemble Methods Evaluated\n",
    "\n",
    "1. Weighted Majority Voting Ensemble\n",
    "\n",
    "   Combines model predictions using fixed assigned weights to each model’s vote.\n",
    "\n",
    "   Formula:\n",
    "\n",
    "   $$\n",
    "   \\text{Final Decision} = \\arg \\max_{c} \\sum_{m=1}^{M} w_m \\cdot \\mathbb{1}_{\\{p_m = c\\}}\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $ w_m $ is the weight of model $ m $\n",
    "   - $ p_m $ is its predicted class\n",
    "   - $ \\mathbb{1}_{\\{p_m = c\\}} $ is an indicator if model $ m $ predicts class $ c $.\n",
    "\n",
    "2. Probabilistic Voting Ensemble (Unconstrained)\n",
    "\n",
    "   Averages the softmax probabilities across all models to derive the final prediction.\n",
    "\n",
    "   Formula:\n",
    "\n",
    "   $$\n",
    "   \\text{Final Decision} = \\arg \\max_{c} \\left( \\frac{1}{M} \\sum_{m=1}^{M} P_m(c) \\right)\n",
    "   $$\n",
    "\n",
    "   where $ P_m(c) $ is the predicted probability for class $ c $ from model $ m $.\n",
    "\n",
    "3. Probabilistic Voting Ensemble (Constrained)\n",
    "\n",
    "   Same as above, but executed with trading constraints such as:\n",
    "\n",
    "   - Maximum position size\n",
    "   - Stop-loss thresholds\n",
    "   - Take-profit thresholds\n",
    "\n",
    "   This reflects more realistic portfolio constraints.\n",
    "\n",
    "4. Probabilistic Voting Ensemble with Kelly Criterion Sizing\n",
    "\n",
    "   Uses averaged probabilistic predictions for signals, and dynamically sizes each trade based on the Kelly criterion calculated from historical win/loss ratios.\n",
    "\n",
    "   Formula:\n",
    "\n",
    "   $$\n",
    "   f^* = \\frac{bp - q}{b}\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $ f^* $: optimal fraction of capital to bet\n",
    "   - $ b $: odds received (average win / average loss)\n",
    "   - $ p $: probability of winning\n",
    "   - $ q = 1 - p $.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "\n",
    "For each ensemble method, we report:\n",
    "\n",
    "- Total Return\n",
    "- Annualized Return\n",
    "- Sharpe Ratio\n",
    "- Sortino Ratio\n",
    "- Maximum Drawdown\n",
    "- Win Rate\n",
    "- Profit Factor\n",
    "\n",
    "Above some of these Evaluation Metrics have been defined below are the rest:\n",
    "\n",
    "#### Calmar Ratio\n",
    "\n",
    "Compares *annualized return to maximum drawdown* to assess return-risk tradeoff.\n",
    "\n",
    "$$\n",
    "\\text{Calmar Ratio} = \\frac{R_a}{|\\text{Max Drawdown}|}\n",
    "$$\n",
    "\n",
    "- $ R_a $: annualized return\n",
    "\n",
    "#### Omega Ratio\n",
    "\n",
    "Considers the *entire distribution of returns relative to a target threshold*.\n",
    "\n",
    "$$\n",
    "\\text{Omega Ratio} = \\frac{\\int_{r_T}^{\\infty} [1 - F(r)] \\, dr}{\\int_{-\\infty}^{r_T} F(r) \\, dr}\n",
    "$$\n",
    "\n",
    "- $ r_T $: target return (e.g. 0)  \n",
    "- $ F(r) $: cumulative distribution function of returns\n",
    "\n",
    "\n",
    "#### Gain to Pain Ratio\n",
    "\n",
    "Measures *total net gains relative to total absolute losses*.\n",
    "\n",
    "$$\n",
    "\\text{Gain to Pain} = \\frac{\\sum \\text{Returns}}{\\sum |\\text{Losses}|}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Profit Factor\n",
    "\n",
    "Calculates *gross profit divided by gross loss*.\n",
    "\n",
    "$$\n",
    "\\text{Profit Factor} = \\frac{\\text{Gross Profit}}{|\\text{Gross Loss}|}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Expectancy per Trade\n",
    "\n",
    "Indicates *average expected profit per trade* accounting for win/loss probabilities.\n",
    "\n",
    "$$\n",
    "\\text{Expectancy} = (\\text{Win Rate} \\times \\text{Avg Win}) - (\\text{Loss Rate} \\times \\text{Avg Loss})\n",
    "$$\n",
    "\n",
    "\n",
    "#### CAGR to Max Drawdown Ratio\n",
    "\n",
    "Compares *compound annual growth rate to maximum drawdown*.\n",
    "\n",
    "$$\n",
    "\\text{CAGR/MDD} = \\frac{\\text{CAGR}}{|\\text{Max Drawdown}|}\n",
    "$$\n",
    "\n",
    "- *CAGR* is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{CAGR} = \\left( \\frac{P_{end}}{P_{start}} \\right)^{\\frac{1}{n}} - 1\n",
    "$$\n",
    "\n",
    "where:\n",
    "  - $ P_{end} $: ending portfolio value  \n",
    "  - $ P_{start} $: starting portfolio value  \n",
    "  - $ n $: number of years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb46372",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "results_weighted_majority, portfolio_weighted_majority, trade_returns_weighted_majority, ensemble_predictions_weighted_majority, df_eval_weighted_majority = run_ensemble_backtest(ticker)\n",
    "results_kelly, portfolio_kelly, trade_returns_kelly, ensemble_predictions_kelly, df_eval_kelly = run_probabilistic_ensemble_backtest_with_kelly(ticker)\n",
    "results_prob_voting_uncon, portfolio_prob_voting_uncon, trade_returns_prob_voting_uncon, ensemble_predictions_prob_voting_uncon, df_eval_prob_voting_uncon = run_probabilistic_ensemble_backtest(ticker, position_size=1, stop_loss=1, take_profit=1)\n",
    "results_prob_voting_con_min_pos, portfolio_prob_voting_con_min_pos, trade_returns_prob_voting_con_min_pos, ensemble_predictions_prob_voting_con_min_pos, df_eval_prob_voting_con_min_pos = run_probabilistic_ensemble_backtest(ticker, position_size=1)\n",
    "results_prob_voting_tot_con, portfolio_prob_voting_tot_con, trade_returns_prob_voting_tot_con, ensemble_predictions_prob_voting_tot_con, df_eval_prob_voting_tot_con = run_probabilistic_ensemble_backtest(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ensemble Model Comparison Summary ===\n",
      "                            Ensemble Method  Total Return  Annualized Return  Sharpe Ratio  Sortino Ratio  Max Drawdown  Win Rate  Profit Factor\n",
      "                   Weighted Majority Voting        2.5765             0.1659        0.5823         0.4820       -0.5828    0.6867         1.7498\n",
      "       Probabilistic Voting (Unconstrained)        1.1019             0.0936        0.4226         0.3604       -0.5484    0.6625         1.4621\n",
      "Probabilistic Voting (Constrained, Min Pos)        0.6302             0.0606        0.3472         0.2851       -0.5484    0.4493         1.1799\n",
      " Probabilistic Voting (Totally Constrained)        0.2870             0.0309        0.3500         0.2881       -0.1682    0.4493         1.1799\n",
      "        Probabilistic Voting + Kelly Sizing       -0.0225            -0.0027       -0.3128        -0.0287       -0.0348    0.3333         0.2599\n"
     ]
    }
   ],
   "source": [
    "metrics_weighted_majority = calculate_additional_metrics(portfolio_weighted_majority, trade_returns_weighted_majority)\n",
    "metrics_kelly = calculate_additional_metrics(portfolio_kelly, trade_returns_kelly)\n",
    "metrics_prob_voting_uncon = calculate_additional_metrics(portfolio_prob_voting_uncon, trade_returns_prob_voting_uncon)\n",
    "metrics_prob_voting_con_min_pos = calculate_additional_metrics(portfolio_prob_voting_con_min_pos, trade_returns_prob_voting_con_min_pos)\n",
    "metrics_prob_voting_tot_con = calculate_additional_metrics(portfolio_prob_voting_tot_con, trade_returns_prob_voting_tot_con)\n",
    "\n",
    "combined_metrics_weighted_majority = {**results_weighted_majority, **metrics_weighted_majority}\n",
    "combined_metrics_kelly = {**results_kelly, **metrics_kelly}\n",
    "combined_metrics_prob_voting_uncon = {**results_prob_voting_uncon, **metrics_prob_voting_uncon}\n",
    "combined_metrics_prob_voting_con_min_pos = {**results_prob_voting_con_min_pos, **metrics_prob_voting_con_min_pos}\n",
    "combined_metrics_prob_voting_tot_con = {**results_prob_voting_tot_con, **metrics_prob_voting_tot_con}\n",
    "\n",
    "table = ensemble_comparison_summary(combined_metrics_weighted_majority, combined_metrics_kelly, combined_metrics_prob_voting_uncon, combined_metrics_prob_voting_con_min_pos, combined_metrics_prob_voting_tot_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924873e",
   "metadata": {},
   "source": [
    "### Conclusion: Ensemble Model Performance\n",
    "\n",
    "This analysis compared multiple ensemble trading strategies across weighted majority voting, probabilistic voting (with and without constraints), and Kelly position sizing.\n",
    "\n",
    "The Weighted Majority Voting ensemble achieved the highest total return (+258%) with a strong profit factor (1.75). However, it suffered from an extremely large maximum drawdown of –58%, indicating significant volatility and potential risk of ruin in real-world deployment.\n",
    "\n",
    "The Probabilistic Voting (Unconstrained) method delivered a solid return (+110%) and good win rate (66%) but maintained drawdowns above –54%, limiting its attractiveness from a risk management perspective.\n",
    "\n",
    "Introducing constraints in probabilistic voting (minimum position sizing and total constraints) progressively reduced returns, with the Totally Constrained variant producing the lowest drawdown (–17%) but also a low annualized return (3.1%). This strategy prioritizes capital preservation over growth.\n",
    "\n",
    "The Kelly Sizing strategy, while theoretically optimal under perfect assumptions, resulted in a slight overall loss (–2.3%) with very low drawdown. This suggests that the inputs for Kelly (estimated win rate and win/loss ratio) were insufficiently calibrated, leading to underbetting or risk-averse position sizes.\n",
    "\n",
    "#### Summary of Tests\n",
    "- Highest return: Weighted Majority Voting (+258%) but with high risk.\n",
    "- Best risk control: Probabilistic Voting (Totally Constrained) with lowest drawdown (–17%) and modest returns.\n",
    "- Kelly sizing underperformed, highlighting the importance of accurate statistical inputs for optimal bet sizing.\n",
    "\n",
    "\n",
    "### Extensions and Future Work\n",
    "\n",
    "This research pipeline lays the foundation for deploying robust, interpretable, and extensible data-driven trading systems. The outlined extensions will transform this framework from an academic backtesting engine into a deployable production-grade quantitative strategy with adaptive risk control, capital allocation, and multi-model integration capabilities. However, while this study has built a robust ensemble-based trading framework with multiple backtested evaluation pipelines, several extensions remain to enhance both predictive accuracy and practical deployment:\n",
    "\n",
    "1. Model Enhancements\n",
    "   - Hyperparameter Optimization: Integrate Bayesian or evolutionary optimization (e.g. Optuna) for systematic tuning of CNN, LSTM, and Transformer architectures.\n",
    "   - Advanced Architectures: Extend to Temporal Fusion Transformers, Attention-based CNN-LSTM hybrids, or modern Normalizing Flows for direct conditional return distribution modeling.\n",
    "\n",
    "2. Feature Expansion\n",
    "   - Incorporate macroeconomic indicators (e.g. FRED datasets) as exogenous features to improve regime awareness.\n",
    "   - Engineer volatility, momentum, and liquidity features beyond RSI and SMA baselines.\n",
    "\n",
    "3. Regime Switching Models\n",
    "   - Combine Hidden Markov Models or unsupervised clustering with ensemble predictions to adapt strategy parameters dynamically based on market conditions.\n",
    "\n",
    "4. Portfolio Context\n",
    "   - Extend the single-asset framework to multi-asset portfolios, including optimal capital allocation across instruments based on expected risk-adjusted returns.\n",
    "\n",
    "5. Risk Management Strategies\n",
    "   - Implement advanced techniques such as dynamic position sizing with volatility scaling, tail risk hedging, and drawdown-based exposure throttling.\n",
    "   - Explore Kelly fraction calibration, Bayesian Kelly approaches, and fractional Kelly to mitigate estimation uncertainty.\n",
    "\n",
    "6. Execution and Slippage Modeling\n",
    "   - Integrate realistic transaction cost models, slippage assumptions, and market impact constraints to simulate execution more accurately.\n",
    "\n",
    "7. Online Learning and Live Deployment\n",
    "   - Develop pipelines for rolling-window online retraining to adapt to changing market dynamics.\n",
    "   - Prepare code for integration with broker APIs (e.g. Interactive Brokers) for live trading and real-time monitoring dashboards.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
