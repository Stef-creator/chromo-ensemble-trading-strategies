{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37997b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# utility functions\n",
    "from utils import *\n",
    "\n",
    "# Models\n",
    "from models.chromosome_model import create_ga_chromosome_metrics\n",
    "from models.cnn_model import *\n",
    "from models.lstm_model import *\n",
    "from models.transformer_model import *\n",
    "from models.mlp_model import *\n",
    "\n",
    "# Scripts\n",
    "from scripts.train_ga import *\n",
    "from scripts.train_mlp import *\n",
    "from scripts.train_cnn import *\n",
    "from scripts.train_lstm import *\n",
    "from scripts.train_transformer import *\n",
    "from scripts.ensemble import *\n",
    "\n",
    "# Set seeds\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Ensure deterministic behavior in PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set environment seed (optional)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc8fcb",
   "metadata": {},
   "source": [
    "# Ensemble Trading Strategy Pipeline\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Data Preparation](#Data-Preparation)\n",
    "3. [Genetic Algorithm Best Chromosome](#Genetic-Algorithm-Best-Chromosome)\n",
    "4. [Model Loading and Predictions](#Model-Loading-and-Predictions)\n",
    "   - 4.1 [MLP Model](#MLP-Model)\n",
    "   - 4.2 [CNN Model](#CNN-Model)\n",
    "   - 4.3 [LSTM Model](#LSTM-Model)\n",
    "   - 4.4 [Transformer Model](#Transformer-Model)\n",
    "5. [Ensemble Voting Strategies](#Ensemble-Voting-Strategies)\n",
    "   - 5.1 [Majority Voting](#Majority-Voting)\n",
    "   - 5.2 [Probabilistic Voting](#Probabilistic-Voting)\n",
    "6. [Backtesting and Evaluation](#Backtesting-and-Evaluation)\n",
    "   - 6.1 [Performance Metrics](#Performance-Metrics)\n",
    "   - 6.2 [Advanced Trading Metrics](#Advanced-Trading-Metrics)\n",
    "7. [Visualization](#Visualization)\n",
    "   - 7.1 [Equity Curve](#Equity-Curve)\n",
    "   - 7.2 [Prediction Signals](#Prediction-Signals)\n",
    "   - 7.3 [Confusion Matrix](#Confusion-Matrix)\n",
    "8. [Conclusions and Next Steps](#Conclusions-and-Next-Steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ebedd",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates a comprehensive ensemble trading strategy pipeline integrating:\n",
    "\n",
    "- Genetic Algorithm optimization for technical indicator parameters\n",
    "- Machine learning models including MLP, CNN, LSTM, and Transformer architectures\n",
    "- Ensemble voting methods, both majority and probabilistic\n",
    "- Advanced backtesting and evaluation metrics for trading performance\n",
    "\n",
    "The goal of this pipeline is to combine multiple predictive models and optimized technical signals to improve trading decision robustness, profitability, and risk-adjusted returns. The models are trained and tuned on historical financial data, and their outputs are evaluated through systematic backtesting with both static and dynamic position sizing strategies.\n",
    "\n",
    "#### Key Features\n",
    "\n",
    "- Genetic Algorithm for RSI parameter tuning  \n",
    "- MLP, CNN, LSTM, and Transformer models with hyperparameter tuning  \n",
    "- Majority voting and probabilistic ensemble integration  \n",
    "- Backtesting with advanced trading performance metrics (Sharpe, Sortino, Calmar, Omega, Gain to Pain, etc.)  \n",
    "- Modular, reproducible pipeline for systematic trading research\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210de2e",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In this section, we load historical financial data for the selected ticker, apply technical indicators, and prepare the dataset for model input.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Download historical price data using the Yahoo Finance API wrapper.\n",
    "2. Calculate technical indicators:\n",
    "   - Relative Strength Index (RSI) over multiple intervals\n",
    "   - Simple Moving Averages (SMA) for trend identification\n",
    "3. Define trend labels based on SMA crossovers to classify uptrends and downtrends.\n",
    "\n",
    "The prepared dataset will be used for generating features and labels required by each machine learning model in the pipeline. Here it is possible to change the ticker to another of your choosing, in this notebook we will be using the Tesla stock ticker.\n",
    "\n",
    "Other examples could be:\n",
    "- ticker = \"BTC-USD\"  # Bitcoin\n",
    "- ticker = \"GLD\"  # SPDR Gold Shares\n",
    "- ticker = \"XLF\"  # Financial sector ETF\n",
    "- ticker = \"TSLA\"  # Tesla, for a high-volatility equity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0484263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved: TSLA_technical_indicators.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ticker = \"TSLA\"\n",
    "start_date = \"1997-01-01\"\n",
    "end_date = \"2017-01-01\"\n",
    "df = download_stock_data(ticker, start_date, end_date)\n",
    "df = add_technical_indicators(df, ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94015279",
   "metadata": {},
   "source": [
    "## Genetic Algorithm Best Chromosome\n",
    "\n",
    "In this section, we load the optimized RSI and interval parameters generated by the Genetic Algorithm (GA) tuning process.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The GA searches for the combination of RSI thresholds and interval lengths that maximize a predefined *fitness function* related to trading performance. These optimized parameters are critical for generating features and labels for model training and evaluation.\n",
    "\n",
    "### Optimization Problem\n",
    "\n",
    "The GA seeks to maximize the *fitness function* defined as:\n",
    "\n",
    "$$\n",
    "\\text{Fitness} = \\frac{R_a}{|\\text{Max Drawdown}|}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ R_a $ is the *annualized return*, calculated as:\n",
    "\n",
    "$$\n",
    "R_a = \\left( \\frac{P_{end}}{P_{start}} \\right)^{\\frac{252}{N}} - 1\n",
    "$$\n",
    "\n",
    "Here:\n",
    "  - $ P_{end} $ = portfolio value at end\n",
    "  - $ P_{start} $ = portfolio value at start\n",
    "  - $ N $ = number of trading days\n",
    "\n",
    "- *Max Drawdown* is the largest observed loss from a peak to a trough before a new peak is reached, calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Max Drawdown} = \\min_t \\left( \\frac{P_t - \\max_{i \\leq t} P_i}{\\max_{i \\leq t} P_i} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Chromosome Representation\n",
    "\n",
    "Each *chromosome* in the GA population encodes a set of RSI-based trading parameters:\n",
    "\n",
    "- Downtrend buy value and interval\n",
    "- Downtrend sell value and interval\n",
    "- Uptrend buy value and interval\n",
    "- Uptrend sell value and interval\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. *Load GA results* from the saved CSV file containing the final tuned chromosomes.\n",
    "2. *Extract the best chromosome* based on the highest fitness score.\n",
    "3. *Prepare these optimized parameters* for feature generation and labeling in the machine learning pipeline.\n",
    "\n",
    "These parameters ensure that models are trained using historically optimal RSI thresholds and intervals, enhancing trading signal relevance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9128195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Genetic Algorithm Chromosome Performance Comparison ===\n",
      " down_buy_val  down_buy_int  down_sell_val  down_sell_int  up_buy_val  up_buy_int  up_sell_val  up_sell_int  total_return  annualized_return  sharpe_ratio  sortino_ratio  max_drawdown  fitness\n",
      "    27.379938             5      85.954267             12   12.812376           8    83.684482            7      1.127715           0.123018      0.554271       0.449230     -0.425369 0.289203\n",
      "    25.667238             6      61.042903             11   13.143131           5    79.643577           18      3.381593           0.254851      0.935329       0.826542     -0.406097 0.627563\n",
      "    12.715422            13      88.330066              5   31.558258          10    84.434879           15      3.359156           0.253862      0.752369       0.937224     -0.458625 0.553528\n",
      "    14.725497            11      93.502458             15    8.577360          17    63.385073           16      1.415102           0.145094      0.882258       0.575734     -0.162858 0.890925\n",
      "    26.130411             6      85.540613              8   39.059052          17    62.758007           14      0.839962           0.098222      0.440931       0.449313     -0.372307 0.263820\n",
      "    34.029163            16      80.207325              7    6.603853          12    87.057392            7      0.721969           0.087095      0.435985       0.362489     -0.347739 0.250459\n",
      "    34.936120             8      73.304418             19   27.248956          16    65.692893           16      2.331691           0.203128      0.761584       0.624891     -0.414881 0.489605\n",
      "    12.332746            13      84.563711              7   26.319585          10    78.694896           12      2.084503           0.188961      0.673180       0.665375     -0.421618 0.448179\n",
      "    10.719087            17      69.448174             12   28.961499           6    68.016683            6      2.852820           0.230297      0.815670       0.755930     -0.401400 0.573734\n",
      "    33.176603            17      69.370931             11   36.959764          15    67.441929           20      1.153056           0.125063      0.511288       0.505325     -0.477670 0.261819\n",
      "\n",
      "Results saved to GA_chromosome_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "create_ga_chromosome_metrics(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55cce5",
   "metadata": {},
   "source": [
    "### Genetic Algorithm Chromosome Performance Interpretation\n",
    "\n",
    "The table below summarizes the performance of multiple chromosomes, each representing a set of RSI thresholds and intervals optimized by the Genetic Algorithm (GA).\n",
    "\n",
    "| Metric | Explanation |\n",
    "|--------|-------------|\n",
    "| down_buy_val / down_buy_int | RSI buy threshold and interval during downtrend |\n",
    "| down_sell_val / down_sell_int | RSI sell threshold and interval during downtrend |\n",
    "| up_buy_val / up_buy_int | RSI buy threshold and interval during uptrend |\n",
    "| up_sell_val / up_sell_int | RSI sell threshold and interval during uptrend |\n",
    "| total_return | Overall return factor (e.g. 1.12 = +12%) |\n",
    "| annualized_return | Annualized return scaled to a yearly basis |\n",
    "| sharpe_ratio | Risk-adjusted return accounting for standard deviation |\n",
    "| sortino_ratio | Risk-adjusted return penalizing only downside volatility |\n",
    "| max_drawdown | Largest peak-to-trough decline observed |\n",
    "| fitness | Defined as annualized return divided by absolute max drawdown |\n",
    "\n",
    "#### Key insights\n",
    "\n",
    "- Highest fitness:  \n",
    "  - Chromosome index 4 with fitness = 0.8909, annualized return = 14.5%, and very low max drawdown = –16.3%.  \n",
    "  - This represents the best risk-adjusted performance under the defined fitness metric.\n",
    "\n",
    "- Highest total return:  \n",
    "  - Chromosome index 2 with total return = 3.38x (238% net gain), annualized return = 25.5%, and max drawdown = –40.6%.  \n",
    "  - While it achieves the highest returns, the drawdown is significantly higher, reducing its fitness relative to index 4.\n",
    "\n",
    "#### Strategic takeaway\n",
    "\n",
    "The optimal chromosome selection depends on trading objectives:\n",
    "\n",
    "- If prioritizing maximum absolute returns, chromosome index 2 is preferable despite higher drawdown.\n",
    "- If prioritizing risk-adjusted returns with controlled drawdown, chromosome index 4 is optimal under the GA fitness definition.\n",
    "\n",
    "These optimized parameters will guide feature generation and labeling for all subsequent machine learning models in the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda66172",
   "metadata": {},
   "source": [
    "## Alternative Fitness Functions for Genetic Algorithm Optimization\n",
    "\n",
    "Below are alternative fitness functions that have been implemented in this code that be used to optimize trading strategy performance, each with its mathematical definition and interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "### Sharpe Ratio\n",
    "\n",
    "Measures *risk-adjusted return*, penalizing both upside and downside volatility.\n",
    "\n",
    "$$\n",
    "\\text{Sharpe Ratio} = \\frac{R_p - R_f}{\\sigma_p}\n",
    "$$\n",
    "\n",
    "- $ R_p $: portfolio return  \n",
    "- $ R_f $: risk-free rate  \n",
    "- $ \\sigma_p $: standard deviation of portfolio returns\n",
    "\n",
    "### Sortino Ratio\n",
    "\n",
    "Adjusts for *downside risk only*, ignoring upside volatility.\n",
    "\n",
    "$$\n",
    "\\text{Sortino Ratio} = \\frac{R_p - R_f}{\\sigma_D}\n",
    "$$\n",
    "\n",
    "- $ \\sigma_D $: standard deviation of negative returns (downside deviation)\n",
    "\n",
    "---\n",
    "\n",
    "### Strategic Note\n",
    "\n",
    "Selecting a fitness function depends on your *trading objectives* (max return, max risk-adjusted return, drawdown minimization) and your *risk tolerance* for the strategy.\n",
    "\n",
    "Consider testing multiple objectives to identify which yields the most robust out-of-sample performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "238cff38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1: Best Fitness = 1.0541\n",
      "Generation 2: Best Fitness = 1.0541\n",
      "  No improvement. Stagnation count: 1/10\n",
      "Generation 3: Best Fitness = 1.1009\n",
      "Generation 4: Best Fitness = 1.1853\n",
      "Generation 5: Best Fitness = 1.1853\n",
      "  No improvement. Stagnation count: 1/10\n",
      "Generation 6: Best Fitness = 1.1853\n",
      "  No improvement. Stagnation count: 2/10\n",
      "Generation 7: Best Fitness = 1.4514\n",
      "Generation 8: Best Fitness = 1.4514\n",
      "  No improvement. Stagnation count: 1/10\n",
      "Generation 9: Best Fitness = 1.4514\n",
      "  No improvement. Stagnation count: 2/10\n",
      "Generation 10: Best Fitness = 1.4514\n",
      "  No improvement. Stagnation count: 3/10\n",
      "Generation 11: Best Fitness = 1.5112\n",
      "Generation 12: Best Fitness = 1.5112\n",
      "  No improvement. Stagnation count: 1/10\n",
      "Generation 13: Best Fitness = 1.5112\n",
      "  No improvement. Stagnation count: 2/10\n",
      "Generation 14: Best Fitness = 1.5112\n",
      "  No improvement. Stagnation count: 3/10\n",
      "Generation 15: Best Fitness = 1.5112\n",
      "  No improvement. Stagnation count: 4/10\n",
      "Generation 16: Best Fitness = 1.5112\n",
      "  No improvement. Stagnation count: 5/10\n",
      "Generation 17: Best Fitness = 1.5112\n",
      "  No improvement. Stagnation count: 6/10\n",
      "Generation 18: Best Fitness = 1.5112\n",
      "  No improvement. Stagnation count: 7/10\n",
      "Generation 19: Best Fitness = 1.5112\n",
      "  No improvement. Stagnation count: 8/10\n",
      "Generation 20: Best Fitness = 1.5112\n",
      "  No improvement. Stagnation count: 9/10\n",
      "Generation 21: Best Fitness = 1.5112\n",
      "  No improvement. Stagnation count: 10/10\n",
      "Convergence reached. Early stopping.\n",
      "\n",
      "Override selection applied. Sorted by sharpe_ratio descending.\n",
      "\n",
      "Final results saved to GA_final_results.csv\n"
     ]
    }
   ],
   "source": [
    "genetic_algorithm(ticker, override_selection_metric='sharpe_ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce926d6",
   "metadata": {},
   "source": [
    "## Model Loading and Predictions\n",
    "\n",
    "In this section, we load each trained machine learning model and generate their predictions on the prepared labeled dataset.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The ensemble pipeline integrates predictions from multiple models to enhance trading signal robustness. Each model was trained using the optimized technical indicator parameters obtained from the Genetic Algorithm and tuned for hyperparameter configurations.\n",
    "\n",
    "### Models included:\n",
    "\n",
    "1. *MLP (Multi-Layer Perceptron)*\n",
    "2. *CNN (Convolutional Neural Network)*\n",
    "3. *LSTM (Long Short-Term Memory Network)*\n",
    "4. *Transformer Model*\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. *Load trained model weights* from saved `.pth` files.\n",
    "2. *Prepare model input data* with correct feature structure.\n",
    "3. *Generate predictions* using each model in evaluation mode.\n",
    "4. *Store predictions for ensemble integration.*\n",
    "\n",
    "The outputs from this section will be used to construct ensemble voting strategies in subsequent analyses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7df05",
   "metadata": {},
   "source": [
    "### MLP Model\n",
    "\n",
    "In this subsection, we load the trained Multi-Layer Perceptron (MLP) model and generate its predictions on the prepared labeled dataset.\n",
    "\n",
    "#### Model overview\n",
    "\n",
    "The *MLP model* is a fully connected feedforward neural network trained to classify trading actions (Buy, Sell, Hold) based on RSI features and trend information.\n",
    "\n",
    "#### Mathematical formulation\n",
    "\n",
    "An MLP with $ L $ layers can be formulated as:\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "& h^{(0)} = x \\\\\n",
    "& h^{(l)} = \\sigma(W^{(l)} h^{(l-1)} + b^{(l)}), \\quad l = 1, \\ldots, L-1 \\\\\n",
    "& \\hat{y} = \\text{softmax}(W^{(L)} h^{(L-1)} + b^{(L)})\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $ x $ is the input feature vector (e.g. RSI value, interval, trend).  \n",
    "- $ W^{(l)} $ and $ b^{(l)} $ are the weights and biases of layer $ l $.  \n",
    "- $ \\sigma $ is the activation function (e.g. ReLU).  \n",
    "- softmax output yields class probabilities for Buy, Sell, Hold.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Load model architecture with the tuned hidden layer configuration.\n",
    "2. Load trained weights from the saved `.pth` file.\n",
    "3. Prepare input tensor with required feature columns.\n",
    "4. Generate predictions and extract class labels for ensemble integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff5d5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing LR=0.001, Batch Size=64, Hidden Sizes=[20, 10, 8, 6, 5]\n",
      "Epoch [1/100], Loss: 1.0544\n",
      "Epoch [20/100], Loss: 0.2437\n",
      "Epoch [40/100], Loss: 0.0776\n",
      "Epoch [60/100], Loss: 0.0599\n",
      "Epoch [80/100], Loss: 0.0452\n",
      "Epoch [100/100], Loss: 0.0409\n",
      "\n",
      "Testing LR=0.001, Batch Size=64, Hidden Sizes=[30, 20, 10]\n",
      "Epoch [1/100], Loss: 1.4262\n",
      "Epoch [20/100], Loss: 0.2561\n",
      "Epoch [40/100], Loss: 0.1050\n",
      "Epoch [60/100], Loss: 0.0790\n",
      "Epoch [80/100], Loss: 0.0560\n",
      "Epoch [100/100], Loss: 0.0433\n",
      "\n",
      "Testing LR=0.001, Batch Size=64, Hidden Sizes=[40, 20]\n",
      "Epoch [1/100], Loss: 1.1517\n",
      "Epoch [20/100], Loss: 0.3399\n",
      "Epoch [40/100], Loss: 0.1613\n",
      "Epoch [60/100], Loss: 0.1113\n",
      "Epoch [80/100], Loss: 0.0829\n",
      "Epoch [100/100], Loss: 0.0706\n",
      "\n",
      "Testing LR=0.001, Batch Size=128, Hidden Sizes=[20, 10, 8, 6, 5]\n",
      "Epoch [1/100], Loss: 1.1170\n",
      "Epoch [20/100], Loss: 0.4482\n",
      "Epoch [40/100], Loss: 0.1892\n",
      "Epoch [60/100], Loss: 0.0929\n",
      "Epoch [80/100], Loss: 0.0650\n",
      "Epoch [100/100], Loss: 0.0568\n",
      "\n",
      "Testing LR=0.001, Batch Size=128, Hidden Sizes=[30, 20, 10]\n",
      "Epoch [1/100], Loss: 1.0603\n",
      "Epoch [20/100], Loss: 0.7657\n",
      "Epoch [40/100], Loss: 0.2108\n",
      "Epoch [60/100], Loss: 0.1049\n",
      "Epoch [80/100], Loss: 0.0805\n",
      "Epoch [100/100], Loss: 0.0590\n",
      "\n",
      "Testing LR=0.001, Batch Size=128, Hidden Sizes=[40, 20]\n",
      "Epoch [1/100], Loss: 0.9654\n",
      "Epoch [20/100], Loss: 0.3464\n",
      "Epoch [40/100], Loss: 0.1504\n",
      "Epoch [60/100], Loss: 0.1056\n",
      "Epoch [80/100], Loss: 0.0688\n",
      "Epoch [100/100], Loss: 0.0596\n",
      "\n",
      "Testing LR=0.001, Batch Size=256, Hidden Sizes=[20, 10, 8, 6, 5]\n",
      "Epoch [1/100], Loss: 1.1454\n",
      "Epoch [20/100], Loss: 1.0665\n",
      "Epoch [40/100], Loss: 0.7037\n",
      "Epoch [60/100], Loss: 0.5781\n",
      "Epoch [80/100], Loss: 0.3881\n",
      "Epoch [100/100], Loss: 0.0969\n",
      "\n",
      "Testing LR=0.001, Batch Size=256, Hidden Sizes=[30, 20, 10]\n",
      "Epoch [1/100], Loss: 1.1902\n",
      "Epoch [20/100], Loss: 0.6953\n",
      "Epoch [40/100], Loss: 0.3153\n",
      "Epoch [60/100], Loss: 0.1762\n",
      "Epoch [80/100], Loss: 0.1208\n",
      "Epoch [100/100], Loss: 0.0932\n",
      "\n",
      "Testing LR=0.001, Batch Size=256, Hidden Sizes=[40, 20]\n",
      "Epoch [1/100], Loss: 2.4981\n",
      "Epoch [20/100], Loss: 0.8233\n",
      "Epoch [40/100], Loss: 0.5383\n",
      "Epoch [60/100], Loss: 0.3762\n",
      "Epoch [80/100], Loss: 0.2647\n",
      "Epoch [100/100], Loss: 0.1692\n",
      "\n",
      "Testing LR=0.0005, Batch Size=64, Hidden Sizes=[20, 10, 8, 6, 5]\n",
      "Epoch [1/100], Loss: 1.1020\n",
      "Epoch [20/100], Loss: 0.5642\n",
      "Epoch [40/100], Loss: 0.1281\n",
      "Epoch [60/100], Loss: 0.0787\n",
      "Epoch [80/100], Loss: 0.0596\n",
      "Epoch [100/100], Loss: 0.0544\n",
      "\n",
      "Testing LR=0.0005, Batch Size=64, Hidden Sizes=[30, 20, 10]\n",
      "Epoch [1/100], Loss: 1.1456\n",
      "Epoch [20/100], Loss: 0.4832\n",
      "Epoch [40/100], Loss: 0.3529\n",
      "Epoch [60/100], Loss: 0.2544\n",
      "Epoch [80/100], Loss: 0.1840\n",
      "Epoch [100/100], Loss: 0.1331\n",
      "\n",
      "Testing LR=0.0005, Batch Size=64, Hidden Sizes=[40, 20]\n",
      "Epoch [1/100], Loss: 1.6190\n",
      "Epoch [20/100], Loss: 0.4875\n",
      "Epoch [40/100], Loss: 0.2418\n",
      "Epoch [60/100], Loss: 0.1558\n",
      "Epoch [80/100], Loss: 0.1153\n",
      "Epoch [100/100], Loss: 0.0886\n",
      "\n",
      "Testing LR=0.0005, Batch Size=128, Hidden Sizes=[20, 10, 8, 6, 5]\n",
      "Epoch [1/100], Loss: 1.1423\n",
      "Epoch [20/100], Loss: 0.9635\n",
      "Epoch [40/100], Loss: 0.7045\n",
      "Epoch [60/100], Loss: 0.4494\n",
      "Epoch [80/100], Loss: 0.2813\n",
      "Epoch [100/100], Loss: 0.2183\n",
      "\n",
      "Testing LR=0.0005, Batch Size=128, Hidden Sizes=[30, 20, 10]\n",
      "Epoch [1/100], Loss: 1.0540\n",
      "Epoch [20/100], Loss: 0.7786\n",
      "Epoch [40/100], Loss: 0.4312\n",
      "Epoch [60/100], Loss: 0.2346\n",
      "Epoch [80/100], Loss: 0.1314\n",
      "Epoch [100/100], Loss: 0.0876\n",
      "\n",
      "Testing LR=0.0005, Batch Size=128, Hidden Sizes=[40, 20]\n",
      "Epoch [1/100], Loss: 2.3711\n",
      "Epoch [20/100], Loss: 0.7596\n",
      "Epoch [40/100], Loss: 0.4636\n",
      "Epoch [60/100], Loss: 0.3125\n",
      "Epoch [80/100], Loss: 0.2348\n",
      "Epoch [100/100], Loss: 0.1815\n",
      "\n",
      "Testing LR=0.0005, Batch Size=256, Hidden Sizes=[20, 10, 8, 6, 5]\n",
      "Epoch [1/100], Loss: 1.0963\n",
      "Epoch [20/100], Loss: 0.9535\n",
      "Epoch [40/100], Loss: 0.8631\n",
      "Epoch [60/100], Loss: 0.7345\n",
      "Epoch [80/100], Loss: 0.5736\n",
      "Epoch [100/100], Loss: 0.4143\n",
      "\n",
      "Testing LR=0.0005, Batch Size=256, Hidden Sizes=[30, 20, 10]\n",
      "Epoch [1/100], Loss: 1.2991\n",
      "Epoch [20/100], Loss: 0.8414\n",
      "Epoch [40/100], Loss: 0.6916\n",
      "Epoch [60/100], Loss: 0.5289\n",
      "Epoch [80/100], Loss: 0.2969\n",
      "Epoch [100/100], Loss: 0.1581\n",
      "\n",
      "Testing LR=0.0005, Batch Size=256, Hidden Sizes=[40, 20]\n",
      "Epoch [1/100], Loss: 4.3957\n",
      "Epoch [20/100], Loss: 0.8741\n",
      "Epoch [40/100], Loss: 0.8003\n",
      "Epoch [60/100], Loss: 0.7273\n",
      "Epoch [80/100], Loss: 0.5802\n",
      "Epoch [100/100], Loss: 0.4376\n",
      "\n",
      "Testing LR=0.0001, Batch Size=64, Hidden Sizes=[20, 10, 8, 6, 5]\n",
      "Epoch [1/100], Loss: 1.0796\n",
      "Epoch [20/100], Loss: 0.9924\n",
      "Epoch [40/100], Loss: 0.8555\n",
      "Epoch [60/100], Loss: 0.7754\n",
      "Epoch [80/100], Loss: 0.7132\n",
      "Epoch [100/100], Loss: 0.6468\n",
      "\n",
      "Testing LR=0.0001, Batch Size=64, Hidden Sizes=[30, 20, 10]\n",
      "Epoch [1/100], Loss: 1.5265\n",
      "Epoch [20/100], Loss: 0.9276\n",
      "Epoch [40/100], Loss: 0.7551\n",
      "Epoch [60/100], Loss: 0.6147\n",
      "Epoch [80/100], Loss: 0.4684\n",
      "Epoch [100/100], Loss: 0.3768\n",
      "\n",
      "Testing LR=0.0001, Batch Size=64, Hidden Sizes=[40, 20]\n",
      "Epoch [1/100], Loss: 2.6941\n",
      "Epoch [20/100], Loss: 0.8904\n",
      "Epoch [40/100], Loss: 0.7801\n",
      "Epoch [60/100], Loss: 0.6663\n",
      "Epoch [80/100], Loss: 0.5694\n",
      "Epoch [100/100], Loss: 0.4835\n",
      "\n",
      "Testing LR=0.0001, Batch Size=128, Hidden Sizes=[20, 10, 8, 6, 5]\n",
      "Epoch [1/100], Loss: 1.0987\n",
      "Epoch [20/100], Loss: 1.0419\n",
      "Epoch [40/100], Loss: 0.9889\n",
      "Epoch [60/100], Loss: 0.9656\n",
      "Epoch [80/100], Loss: 0.9497\n",
      "Epoch [100/100], Loss: 0.9254\n",
      "\n",
      "Testing LR=0.0001, Batch Size=128, Hidden Sizes=[30, 20, 10]\n",
      "Epoch [1/100], Loss: 1.0213\n",
      "Epoch [20/100], Loss: 0.9452\n",
      "Epoch [40/100], Loss: 0.8851\n",
      "Epoch [60/100], Loss: 0.8286\n",
      "Epoch [80/100], Loss: 0.7729\n",
      "Epoch [100/100], Loss: 0.6914\n",
      "\n",
      "Testing LR=0.0001, Batch Size=128, Hidden Sizes=[40, 20]\n",
      "Epoch [1/100], Loss: 2.2060\n",
      "Epoch [20/100], Loss: 0.9677\n",
      "Epoch [40/100], Loss: 0.8368\n",
      "Epoch [60/100], Loss: 0.7568\n",
      "Epoch [80/100], Loss: 0.7022\n",
      "Epoch [100/100], Loss: 0.6459\n",
      "\n",
      "Testing LR=0.0001, Batch Size=256, Hidden Sizes=[20, 10, 8, 6, 5]\n",
      "Epoch [1/100], Loss: 1.1212\n",
      "Epoch [20/100], Loss: 1.0988\n",
      "Epoch [40/100], Loss: 1.0719\n",
      "Epoch [60/100], Loss: 1.0574\n",
      "Epoch [80/100], Loss: 1.0387\n",
      "Epoch [100/100], Loss: 1.0278\n",
      "\n",
      "Testing LR=0.0001, Batch Size=256, Hidden Sizes=[30, 20, 10]\n",
      "Epoch [1/100], Loss: 1.1196\n",
      "Epoch [20/100], Loss: 1.0134\n",
      "Epoch [40/100], Loss: 0.9450\n",
      "Epoch [60/100], Loss: 0.9092\n",
      "Epoch [80/100], Loss: 0.8880\n",
      "Epoch [100/100], Loss: 0.8618\n",
      "\n",
      "Testing LR=0.0001, Batch Size=256, Hidden Sizes=[40, 20]\n",
      "Epoch [1/100], Loss: 1.5898\n",
      "Epoch [20/100], Loss: 1.0509\n",
      "Epoch [40/100], Loss: 0.9830\n",
      "Epoch [60/100], Loss: 0.9354\n",
      "Epoch [80/100], Loss: 0.9050\n",
      "Epoch [100/100], Loss: 0.8712\n",
      "\n",
      "=== Hyperparameter Tuning Results ===\n",
      "    learning_rate  batch_size       hidden_sizes  avg_loss\n",
      "0          0.0010          64  [20, 10, 8, 6, 5]  0.040935\n",
      "1          0.0010          64       [30, 20, 10]  0.043292\n",
      "9          0.0005          64  [20, 10, 8, 6, 5]  0.054371\n",
      "3          0.0010         128  [20, 10, 8, 6, 5]  0.056764\n",
      "4          0.0010         128       [30, 20, 10]  0.059045\n",
      "5          0.0010         128           [40, 20]  0.059585\n",
      "2          0.0010          64           [40, 20]  0.070556\n",
      "13         0.0005         128       [30, 20, 10]  0.087626\n",
      "11         0.0005          64           [40, 20]  0.088621\n",
      "7          0.0010         256       [30, 20, 10]  0.093155\n",
      "6          0.0010         256  [20, 10, 8, 6, 5]  0.096904\n",
      "10         0.0005          64       [30, 20, 10]  0.133087\n",
      "16         0.0005         256       [30, 20, 10]  0.158071\n",
      "8          0.0010         256           [40, 20]  0.169230\n",
      "14         0.0005         128           [40, 20]  0.181549\n",
      "12         0.0005         128  [20, 10, 8, 6, 5]  0.218251\n",
      "19         0.0001          64       [30, 20, 10]  0.376805\n",
      "15         0.0005         256  [20, 10, 8, 6, 5]  0.414309\n",
      "17         0.0005         256           [40, 20]  0.437598\n",
      "20         0.0001          64           [40, 20]  0.483523\n",
      "23         0.0001         128           [40, 20]  0.645927\n",
      "18         0.0001          64  [20, 10, 8, 6, 5]  0.646839\n",
      "22         0.0001         128       [30, 20, 10]  0.691369\n",
      "25         0.0001         256       [30, 20, 10]  0.861777\n",
      "26         0.0001         256           [40, 20]  0.871174\n",
      "21         0.0001         128  [20, 10, 8, 6, 5]  0.925449\n",
      "24         0.0001         256  [20, 10, 8, 6, 5]  1.027797\n",
      "Epoch [1/200], Loss: 1.1539\n",
      "Epoch [20/200], Loss: 0.4689\n",
      "Epoch [40/200], Loss: 0.3056\n",
      "Epoch [60/200], Loss: 0.2573\n",
      "Epoch [80/200], Loss: 0.2306\n",
      "Epoch [100/200], Loss: 0.2044\n",
      "Epoch [120/200], Loss: 0.1841\n",
      "Epoch [140/200], Loss: 0.1605\n",
      "Epoch [160/200], Loss: 0.1428\n",
      "Epoch [180/200], Loss: 0.1195\n",
      "Epoch [200/200], Loss: 0.0952\n",
      "\n",
      "Model saved as trained_mlp_final.pth\n"
     ]
    }
   ],
   "source": [
    "run_tuning_mlp(ticker)\n",
    "best_mlp(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ac505",
   "metadata": {},
   "source": [
    "### CNN Model\n",
    "\n",
    "In this subsection, we load the trained Convolutional Neural Network (CNN) model and generate its predictions on the prepared labeled dataset.\n",
    "\n",
    "#### Model overview\n",
    "\n",
    "The CNN model captures local temporal patterns in RSI and trend features by applying convolutional filters over input sequences, learning hierarchical representations relevant for trading signal classification.\n",
    "\n",
    "#### Mathematical formulation\n",
    "\n",
    "For a 1D CNN layer:\n",
    "\n",
    "$$\n",
    "h^{(l)}_i = \\sigma \\left( \\sum_{k=0}^{K-1} W_k^{(l)} x_{i+k} + b^{(l)} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ x $ is the input sequence.  \n",
    "- $ W_k^{(l)} $ are the convolutional kernel weights of size $ K $.  \n",
    "- $ b^{(l)} $ is the bias term.  \n",
    "- $ \\sigma $ is the activation function (e.g. ReLU).  \n",
    "- $ h^{(l)}_i $ is the output feature map at position $ i $ in layer $ l $.\n",
    "\n",
    "The final output is passed through fully connected layers and a softmax function to produce class probabilities for Buy, Sell, Hold.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Load model architecture with tuned kernel size and hidden channels.  \n",
    "2. Load trained weights from the saved `.pth` file.  \n",
    "3. Prepare input tensors as rolling window sequences.  \n",
    "4. Generate predictions and extract class labels for ensemble integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905c972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Kernel=2, Hidden Channels=8, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.7898\n",
      "Epoch [20/50], Loss: 0.7296\n",
      "Epoch [30/50], Loss: 0.7005\n",
      "Epoch [40/50], Loss: 0.6960\n",
      "Epoch [50/50], Loss: 0.6794\n",
      "Testing Kernel=2, Hidden Channels=8, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7340\n",
      "Epoch [20/50], Loss: 0.6976\n",
      "Epoch [30/50], Loss: 0.6731\n",
      "Epoch [40/50], Loss: 0.6537\n",
      "Epoch [50/50], Loss: 0.6371\n",
      "Testing Kernel=2, Hidden Channels=8, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.7685\n",
      "Epoch [20/50], Loss: 0.7229\n",
      "Epoch [30/50], Loss: 0.7031\n",
      "Epoch [40/50], Loss: 0.6745\n",
      "Epoch [50/50], Loss: 0.6743\n",
      "Testing Kernel=2, Hidden Channels=8, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7433\n",
      "Epoch [20/50], Loss: 0.7073\n",
      "Epoch [30/50], Loss: 0.6831\n",
      "Epoch [40/50], Loss: 0.6731\n",
      "Epoch [50/50], Loss: 0.6670\n",
      "Testing Kernel=2, Hidden Channels=16, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.7530\n",
      "Epoch [20/50], Loss: 0.6992\n",
      "Epoch [30/50], Loss: 0.6800\n",
      "Epoch [40/50], Loss: 0.6531\n",
      "Epoch [50/50], Loss: 0.6453\n",
      "Testing Kernel=2, Hidden Channels=16, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7339\n",
      "Epoch [20/50], Loss: 0.6963\n",
      "Epoch [30/50], Loss: 0.6763\n",
      "Epoch [40/50], Loss: 0.6640\n",
      "Epoch [50/50], Loss: 0.6670\n",
      "Testing Kernel=2, Hidden Channels=16, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.7444\n",
      "Epoch [20/50], Loss: 0.6947\n",
      "Epoch [30/50], Loss: 0.6728\n",
      "Epoch [40/50], Loss: 0.6593\n",
      "Epoch [50/50], Loss: 0.6463\n",
      "Testing Kernel=2, Hidden Channels=16, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7876\n",
      "Epoch [20/50], Loss: 0.7205\n",
      "Epoch [30/50], Loss: 0.6808\n",
      "Epoch [40/50], Loss: 0.6790\n",
      "Epoch [50/50], Loss: 0.6631\n",
      "Testing Kernel=2, Hidden Channels=32, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.7137\n",
      "Epoch [20/50], Loss: 0.6689\n",
      "Epoch [30/50], Loss: 0.6512\n",
      "Epoch [40/50], Loss: 0.6302\n",
      "Epoch [50/50], Loss: 0.6244\n",
      "Testing Kernel=2, Hidden Channels=32, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7188\n",
      "Epoch [20/50], Loss: 0.7057\n",
      "Epoch [30/50], Loss: 0.6556\n",
      "Epoch [40/50], Loss: 0.6298\n",
      "Epoch [50/50], Loss: 0.6220\n",
      "Testing Kernel=2, Hidden Channels=32, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.7162\n",
      "Epoch [20/50], Loss: 0.6812\n",
      "Epoch [30/50], Loss: 0.6571\n",
      "Epoch [40/50], Loss: 0.6487\n",
      "Epoch [50/50], Loss: 0.6434\n",
      "Testing Kernel=2, Hidden Channels=32, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7396\n",
      "Epoch [20/50], Loss: 0.7013\n",
      "Epoch [30/50], Loss: 0.6978\n",
      "Epoch [40/50], Loss: 0.6763\n",
      "Epoch [50/50], Loss: 0.6713\n",
      "Testing Kernel=3, Hidden Channels=8, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.6885\n",
      "Epoch [20/50], Loss: 0.6363\n",
      "Epoch [30/50], Loss: 0.6052\n",
      "Epoch [40/50], Loss: 0.5948\n",
      "Epoch [50/50], Loss: 0.5699\n",
      "Testing Kernel=3, Hidden Channels=8, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7235\n",
      "Epoch [20/50], Loss: 0.6566\n",
      "Epoch [30/50], Loss: 0.6330\n",
      "Epoch [40/50], Loss: 0.6469\n",
      "Epoch [50/50], Loss: 0.6050\n",
      "Testing Kernel=3, Hidden Channels=8, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.6934\n",
      "Epoch [20/50], Loss: 0.6479\n",
      "Epoch [30/50], Loss: 0.6293\n",
      "Epoch [40/50], Loss: 0.6142\n",
      "Epoch [50/50], Loss: 0.5988\n",
      "Testing Kernel=3, Hidden Channels=8, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7546\n",
      "Epoch [20/50], Loss: 0.6846\n",
      "Epoch [30/50], Loss: 0.6669\n",
      "Epoch [40/50], Loss: 0.6588\n",
      "Epoch [50/50], Loss: 0.6514\n",
      "Testing Kernel=3, Hidden Channels=16, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.7132\n",
      "Epoch [20/50], Loss: 0.6395\n",
      "Epoch [30/50], Loss: 0.6081\n",
      "Epoch [40/50], Loss: 0.5856\n",
      "Epoch [50/50], Loss: 0.5686\n",
      "Testing Kernel=3, Hidden Channels=16, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7806\n",
      "Epoch [20/50], Loss: 0.6962\n",
      "Epoch [30/50], Loss: 0.6666\n",
      "Epoch [40/50], Loss: 0.6567\n",
      "Epoch [50/50], Loss: 0.6493\n",
      "Testing Kernel=3, Hidden Channels=16, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.7470\n",
      "Epoch [20/50], Loss: 0.6895\n",
      "Epoch [30/50], Loss: 0.6495\n",
      "Epoch [40/50], Loss: 0.6283\n",
      "Epoch [50/50], Loss: 0.6238\n",
      "Testing Kernel=3, Hidden Channels=16, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7740\n",
      "Epoch [20/50], Loss: 0.7092\n",
      "Epoch [30/50], Loss: 0.6594\n",
      "Epoch [40/50], Loss: 0.6441\n",
      "Epoch [50/50], Loss: 0.6234\n",
      "Testing Kernel=3, Hidden Channels=32, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.7048\n",
      "Epoch [20/50], Loss: 0.6565\n",
      "Epoch [30/50], Loss: 0.5982\n",
      "Epoch [40/50], Loss: 0.5789\n",
      "Epoch [50/50], Loss: 0.5515\n",
      "Testing Kernel=3, Hidden Channels=32, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7219\n",
      "Epoch [20/50], Loss: 0.6604\n",
      "Epoch [30/50], Loss: 0.6160\n",
      "Epoch [40/50], Loss: 0.6182\n",
      "Epoch [50/50], Loss: 0.5965\n",
      "Testing Kernel=3, Hidden Channels=32, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.7236\n",
      "Epoch [20/50], Loss: 0.6615\n",
      "Epoch [30/50], Loss: 0.6225\n",
      "Epoch [40/50], Loss: 0.6036\n",
      "Epoch [50/50], Loss: 0.5833\n",
      "Testing Kernel=3, Hidden Channels=32, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7423\n",
      "Epoch [20/50], Loss: 0.7052\n",
      "Epoch [30/50], Loss: 0.6689\n",
      "Epoch [40/50], Loss: 0.6603\n",
      "Epoch [50/50], Loss: 0.6323\n",
      "Testing Kernel=5, Hidden Channels=8, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.5977\n",
      "Epoch [20/50], Loss: 0.5266\n",
      "Epoch [30/50], Loss: 0.4825\n",
      "Epoch [40/50], Loss: 0.4575\n",
      "Epoch [50/50], Loss: 0.4160\n",
      "Testing Kernel=5, Hidden Channels=8, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.6203\n",
      "Epoch [20/50], Loss: 0.5496\n",
      "Epoch [30/50], Loss: 0.5139\n",
      "Epoch [40/50], Loss: 0.5094\n",
      "Epoch [50/50], Loss: 0.4617\n",
      "Testing Kernel=5, Hidden Channels=8, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.6578\n",
      "Epoch [20/50], Loss: 0.5876\n",
      "Epoch [30/50], Loss: 0.5534\n",
      "Epoch [40/50], Loss: 0.5218\n",
      "Epoch [50/50], Loss: 0.5055\n",
      "Testing Kernel=5, Hidden Channels=8, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7016\n",
      "Epoch [20/50], Loss: 0.6176\n",
      "Epoch [30/50], Loss: 0.5890\n",
      "Epoch [40/50], Loss: 0.5396\n",
      "Epoch [50/50], Loss: 0.5196\n",
      "Testing Kernel=5, Hidden Channels=16, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.5917\n",
      "Epoch [20/50], Loss: 0.5246\n",
      "Epoch [30/50], Loss: 0.4737\n",
      "Epoch [40/50], Loss: 0.4352\n",
      "Epoch [50/50], Loss: 0.3801\n",
      "Testing Kernel=5, Hidden Channels=16, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.6057\n",
      "Epoch [20/50], Loss: 0.5233\n",
      "Epoch [30/50], Loss: 0.4796\n",
      "Epoch [40/50], Loss: 0.4767\n",
      "Epoch [50/50], Loss: 0.4271\n",
      "Testing Kernel=5, Hidden Channels=16, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.6067\n",
      "Epoch [20/50], Loss: 0.5503\n",
      "Epoch [30/50], Loss: 0.5162\n",
      "Epoch [40/50], Loss: 0.4816\n",
      "Epoch [50/50], Loss: 0.4687\n",
      "Testing Kernel=5, Hidden Channels=16, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.6501\n",
      "Epoch [20/50], Loss: 0.5727\n",
      "Epoch [30/50], Loss: 0.5320\n",
      "Epoch [40/50], Loss: 0.5006\n",
      "Epoch [50/50], Loss: 0.4815\n",
      "Testing Kernel=5, Hidden Channels=32, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.5734\n",
      "Epoch [20/50], Loss: 0.4801\n",
      "Epoch [30/50], Loss: 0.4179\n",
      "Epoch [40/50], Loss: 0.3529\n",
      "Epoch [50/50], Loss: 0.2850\n",
      "Testing Kernel=5, Hidden Channels=32, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.6227\n",
      "Epoch [20/50], Loss: 0.5448\n",
      "Epoch [30/50], Loss: 0.5017\n",
      "Epoch [40/50], Loss: 0.4599\n",
      "Epoch [50/50], Loss: 0.4252\n",
      "Testing Kernel=5, Hidden Channels=32, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.6003\n",
      "Epoch [20/50], Loss: 0.5336\n",
      "Epoch [30/50], Loss: 0.4742\n",
      "Epoch [40/50], Loss: 0.4664\n",
      "Epoch [50/50], Loss: 0.4286\n",
      "Testing Kernel=5, Hidden Channels=32, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.6381\n",
      "Epoch [20/50], Loss: 0.5504\n",
      "Epoch [30/50], Loss: 0.5121\n",
      "Epoch [40/50], Loss: 0.4746\n",
      "Epoch [50/50], Loss: 0.4409\n",
      "\n",
      "=== CNN Tuning Results ===\n",
      " kernel_size  hidden_channels  learning_rate  batch_size  avg_loss\n",
      "           5               32         0.0010          32  0.284985\n",
      "           5               16         0.0010          32  0.380087\n",
      "           5                8         0.0010          32  0.416038\n",
      "           5               32         0.0010          64  0.425212\n",
      "           5               16         0.0010          64  0.427094\n",
      "           5               32         0.0005          32  0.428613\n",
      "           5               32         0.0005          64  0.440938\n",
      "           5                8         0.0010          64  0.461724\n",
      "           5               16         0.0005          32  0.468740\n",
      "           5               16         0.0005          64  0.481509\n",
      "           5                8         0.0005          32  0.505527\n",
      "           5                8         0.0005          64  0.519563\n",
      "           3               32         0.0010          32  0.551469\n",
      "           3               16         0.0010          32  0.568562\n",
      "           3                8         0.0010          32  0.569861\n",
      "           3               32         0.0005          32  0.583347\n",
      "           3               32         0.0010          64  0.596510\n",
      "           3                8         0.0005          32  0.598824\n",
      "           3                8         0.0010          64  0.604993\n",
      "           2               32         0.0010          64  0.621959\n",
      "           3               16         0.0005          64  0.623425\n",
      "           3               16         0.0005          32  0.623837\n",
      "           2               32         0.0010          32  0.624391\n",
      "           3               32         0.0005          64  0.632328\n",
      "           2                8         0.0010          64  0.637141\n",
      "           2               32         0.0005          32  0.643411\n",
      "           2               16         0.0010          32  0.645286\n",
      "           2               16         0.0005          32  0.646347\n",
      "           3               16         0.0010          64  0.649265\n",
      "           3                8         0.0005          64  0.651392\n",
      "           2               16         0.0005          64  0.663050\n",
      "           2               16         0.0010          64  0.666976\n",
      "           2                8         0.0005          64  0.666991\n",
      "           2               32         0.0005          64  0.671346\n",
      "           2                8         0.0005          32  0.674344\n",
      "           2                8         0.0010          32  0.679378\n",
      "\n",
      "Retraining final CNN with best config: Kernel=5.0, Hidden Channels=32.0, LR=0.001, Batch Size=32.0\n",
      "[Final Retrain] Epoch [10/100], Loss: 0.6103\n",
      "[Final Retrain] Epoch [20/100], Loss: 0.5145\n",
      "[Final Retrain] Epoch [30/100], Loss: 0.4637\n",
      "[Final Retrain] Epoch [40/100], Loss: 0.4217\n",
      "[Final Retrain] Epoch [50/100], Loss: 0.3583\n",
      "[Final Retrain] Epoch [60/100], Loss: 0.3244\n",
      "[Final Retrain] Epoch [70/100], Loss: 0.2816\n",
      "[Final Retrain] Epoch [80/100], Loss: 0.2532\n",
      "[Final Retrain] Epoch [90/100], Loss: 0.2339\n",
      "[Final Retrain] Epoch [100/100], Loss: 0.1630\n",
      "\n",
      "Final tuned CNN model saved as optimized_models/trained_cnn_best_tuned.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNNClassifier(\n",
       "  (conv1): Conv1d(3, 32, kernel_size=(5,), stride=(1,))\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
       "  (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_cnn_hyperparameters(ticker)\n",
    "best_cnn(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a17496",
   "metadata": {},
   "source": [
    "### 4.3 LSTM Model\n",
    "\n",
    "In this subsection, we load the trained Long Short-Term Memory (LSTM) model and generate its predictions on the prepared labeled dataset.\n",
    "\n",
    "#### Model overview\n",
    "\n",
    "The LSTM model is a recurrent neural network architecture designed to capture sequential dependencies and temporal patterns in time series data, addressing the vanishing gradient problem present in traditional RNNs.\n",
    "\n",
    "#### Mathematical formulation\n",
    "\n",
    "An LSTM cell operates with the following equations at each time step $ t $:\n",
    "\n",
    "\\begin{aligned}\n",
    "& f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f) \\\\\n",
    "& i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i) \\\\\n",
    "& o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o) \\\\\n",
    "& \\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c) \\\\\n",
    "& c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\\\\n",
    "& h_t = o_t \\odot \\tanh(c_t)\n",
    "\\end{aligned}\n",
    "\n",
    "where:\n",
    "\n",
    "- $ x_t $ is the input vector at time $ t $.  \n",
    "- $ h_{t-1} $ is the previous hidden state.  \n",
    "- $ c_t $ is the cell state.  \n",
    "- $ f_t, i_t, o_t $ are the forget, input, and output gates, respectively.  \n",
    "- $ \\tilde{c}_t $ is the candidate cell state.  \n",
    "- $ W $ and $ U $ are weight matrices, $ b $ are biases.  \n",
    "- $ \\sigma $ denotes the sigmoid activation function, and $ \\odot $ denotes element-wise multiplication.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Load model architecture with tuned hidden size and number of layers.  \n",
    "2. Load trained weights from the saved `.pth` file.  \n",
    "3. Prepare input tensors as sequential rolling windows.  \n",
    "4. Generate predictions and extract class labels for ensemble integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0637e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Hidden Size=16, Num Layers=1, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.6334\n",
      "Epoch [20/50], Loss: 0.5853\n",
      "Epoch [30/50], Loss: 0.5620\n",
      "Epoch [40/50], Loss: 0.5547\n",
      "Epoch [50/50], Loss: 0.5505\n",
      "Testing Hidden Size=16, Num Layers=1, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.6731\n",
      "Epoch [20/50], Loss: 0.5902\n",
      "Epoch [30/50], Loss: 0.5662\n",
      "Epoch [40/50], Loss: 0.5555\n",
      "Epoch [50/50], Loss: 0.5479\n",
      "Testing Hidden Size=16, Num Layers=1, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.7553\n",
      "Epoch [20/50], Loss: 0.6273\n",
      "Epoch [30/50], Loss: 0.5897\n",
      "Epoch [40/50], Loss: 0.5718\n",
      "Epoch [50/50], Loss: 0.5601\n",
      "Testing Hidden Size=16, Num Layers=1, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.8111\n",
      "Epoch [20/50], Loss: 0.6292\n",
      "Epoch [30/50], Loss: 0.5901\n",
      "Epoch [40/50], Loss: 0.5791\n",
      "Epoch [50/50], Loss: 0.5753\n",
      "Testing Hidden Size=16, Num Layers=2, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.5952\n",
      "Epoch [20/50], Loss: 0.5630\n",
      "Epoch [30/50], Loss: 0.5511\n",
      "Epoch [40/50], Loss: 0.5377\n",
      "Epoch [50/50], Loss: 0.5331\n",
      "Testing Hidden Size=16, Num Layers=2, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.6527\n",
      "Epoch [20/50], Loss: 0.5710\n",
      "Epoch [30/50], Loss: 0.5552\n",
      "Epoch [40/50], Loss: 0.5372\n",
      "Epoch [50/50], Loss: 0.5264\n",
      "Testing Hidden Size=16, Num Layers=2, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.6757\n",
      "Epoch [20/50], Loss: 0.5843\n",
      "Epoch [30/50], Loss: 0.5637\n",
      "Epoch [40/50], Loss: 0.5500\n",
      "Epoch [50/50], Loss: 0.5443\n",
      "Testing Hidden Size=16, Num Layers=2, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.7361\n",
      "Epoch [20/50], Loss: 0.6360\n",
      "Epoch [30/50], Loss: 0.5911\n",
      "Epoch [40/50], Loss: 0.5716\n",
      "Epoch [50/50], Loss: 0.5483\n",
      "Testing Hidden Size=32, Num Layers=1, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.5813\n",
      "Epoch [20/50], Loss: 0.5585\n",
      "Epoch [30/50], Loss: 0.5538\n",
      "Epoch [40/50], Loss: 0.5355\n",
      "Epoch [50/50], Loss: 0.5318\n",
      "Testing Hidden Size=32, Num Layers=1, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.6171\n",
      "Epoch [20/50], Loss: 0.5726\n",
      "Epoch [30/50], Loss: 0.5543\n",
      "Epoch [40/50], Loss: 0.5383\n",
      "Epoch [50/50], Loss: 0.5186\n",
      "Testing Hidden Size=32, Num Layers=1, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.6098\n",
      "Epoch [20/50], Loss: 0.5714\n",
      "Epoch [30/50], Loss: 0.5591\n",
      "Epoch [40/50], Loss: 0.5512\n",
      "Epoch [50/50], Loss: 0.5375\n",
      "Testing Hidden Size=32, Num Layers=1, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.6719\n",
      "Epoch [20/50], Loss: 0.5975\n",
      "Epoch [30/50], Loss: 0.5724\n",
      "Epoch [40/50], Loss: 0.5604\n",
      "Epoch [50/50], Loss: 0.5419\n",
      "Testing Hidden Size=32, Num Layers=2, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.5673\n",
      "Epoch [20/50], Loss: 0.5461\n",
      "Epoch [30/50], Loss: 0.5138\n",
      "Epoch [40/50], Loss: 0.5014\n",
      "Epoch [50/50], Loss: 0.4727\n",
      "Testing Hidden Size=32, Num Layers=2, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.5717\n",
      "Epoch [20/50], Loss: 0.5542\n",
      "Epoch [30/50], Loss: 0.5329\n",
      "Epoch [40/50], Loss: 0.5176\n",
      "Epoch [50/50], Loss: 0.4864\n",
      "Testing Hidden Size=32, Num Layers=2, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.5802\n",
      "Epoch [20/50], Loss: 0.5492\n",
      "Epoch [30/50], Loss: 0.5242\n",
      "Epoch [40/50], Loss: 0.5124\n",
      "Epoch [50/50], Loss: 0.4921\n",
      "Testing Hidden Size=32, Num Layers=2, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.6195\n",
      "Epoch [20/50], Loss: 0.5683\n",
      "Epoch [30/50], Loss: 0.5416\n",
      "Epoch [40/50], Loss: 0.5376\n",
      "Epoch [50/50], Loss: 0.5296\n",
      "Testing Hidden Size=64, Num Layers=1, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.5557\n",
      "Epoch [20/50], Loss: 0.5316\n",
      "Epoch [30/50], Loss: 0.5098\n",
      "Epoch [40/50], Loss: 0.4798\n",
      "Epoch [50/50], Loss: 0.4551\n",
      "Testing Hidden Size=64, Num Layers=1, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.5711\n",
      "Epoch [20/50], Loss: 0.5405\n",
      "Epoch [30/50], Loss: 0.5345\n",
      "Epoch [40/50], Loss: 0.4971\n",
      "Epoch [50/50], Loss: 0.4823\n",
      "Testing Hidden Size=64, Num Layers=1, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.5880\n",
      "Epoch [20/50], Loss: 0.5580\n",
      "Epoch [30/50], Loss: 0.5399\n",
      "Epoch [40/50], Loss: 0.5201\n",
      "Epoch [50/50], Loss: 0.5066\n",
      "Testing Hidden Size=64, Num Layers=1, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.6006\n",
      "Epoch [20/50], Loss: 0.5616\n",
      "Epoch [30/50], Loss: 0.5496\n",
      "Epoch [40/50], Loss: 0.5245\n",
      "Epoch [50/50], Loss: 0.5065\n",
      "Testing Hidden Size=64, Num Layers=2, LR=0.001, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.5529\n",
      "Epoch [20/50], Loss: 0.5139\n",
      "Epoch [30/50], Loss: 0.4633\n",
      "Epoch [40/50], Loss: 0.3977\n",
      "Epoch [50/50], Loss: 0.3189\n",
      "Testing Hidden Size=64, Num Layers=2, LR=0.001, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.5576\n",
      "Epoch [20/50], Loss: 0.5365\n",
      "Epoch [30/50], Loss: 0.4981\n",
      "Epoch [40/50], Loss: 0.4642\n",
      "Epoch [50/50], Loss: 0.4273\n",
      "Testing Hidden Size=64, Num Layers=2, LR=0.0005, Batch Size=32\n",
      "Epoch [10/50], Loss: 0.5603\n",
      "Epoch [20/50], Loss: 0.5382\n",
      "Epoch [30/50], Loss: 0.5122\n",
      "Epoch [40/50], Loss: 0.4862\n",
      "Epoch [50/50], Loss: 0.4492\n",
      "Testing Hidden Size=64, Num Layers=2, LR=0.0005, Batch Size=64\n",
      "Epoch [10/50], Loss: 0.5805\n",
      "Epoch [20/50], Loss: 0.5464\n",
      "Epoch [30/50], Loss: 0.5347\n",
      "Epoch [40/50], Loss: 0.5015\n",
      "Epoch [50/50], Loss: 0.4735\n",
      "\n",
      "=== LSTM Tuning Results ===\n",
      " hidden_size  num_layers  learning_rate  batch_size  avg_loss\n",
      "          64           2         0.0010          32  0.318851\n",
      "          64           2         0.0010          64  0.427251\n",
      "          64           2         0.0005          32  0.449242\n",
      "          64           1         0.0010          32  0.455057\n",
      "          32           2         0.0010          32  0.472698\n",
      "          64           2         0.0005          64  0.473476\n",
      "          64           1         0.0010          64  0.482348\n",
      "          32           2         0.0010          64  0.486362\n",
      "          32           2         0.0005          32  0.492124\n",
      "          64           1         0.0005          64  0.506457\n",
      "          64           1         0.0005          32  0.506556\n",
      "          32           1         0.0010          64  0.518568\n",
      "          16           2         0.0010          64  0.526409\n",
      "          32           2         0.0005          64  0.529556\n",
      "          32           1         0.0010          32  0.531781\n",
      "          16           2         0.0010          32  0.533136\n",
      "          32           1         0.0005          32  0.537522\n",
      "          32           1         0.0005          64  0.541890\n",
      "          16           2         0.0005          32  0.544336\n",
      "          16           1         0.0010          64  0.547868\n",
      "          16           2         0.0005          64  0.548336\n",
      "          16           1         0.0010          32  0.550475\n",
      "          16           1         0.0005          32  0.560088\n",
      "          16           1         0.0005          64  0.575304\n",
      "\n",
      "Retraining final LSTM with best config: Hidden Size=64.0, Num Layers=2.0, LR=0.001, Batch Size=32.0\n",
      "[Final Retrain] Epoch [10/100], Loss: 0.5496\n",
      "[Final Retrain] Epoch [20/100], Loss: 0.5097\n",
      "[Final Retrain] Epoch [30/100], Loss: 0.4767\n",
      "[Final Retrain] Epoch [40/100], Loss: 0.4083\n",
      "[Final Retrain] Epoch [50/100], Loss: 0.3446\n",
      "[Final Retrain] Epoch [60/100], Loss: 0.2647\n",
      "[Final Retrain] Epoch [70/100], Loss: 0.1835\n",
      "[Final Retrain] Epoch [80/100], Loss: 0.1153\n",
      "[Final Retrain] Epoch [90/100], Loss: 0.0825\n",
      "[Final Retrain] Epoch [100/100], Loss: 0.0464\n",
      "\n",
      "Final tuned LSTM model saved as optimized_models/trained_lstm_best_tuned.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (lstm): LSTM(3, 64, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_lstm_hyperparameters(ticker)\n",
    "best_lstm(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11055815",
   "metadata": {},
   "source": [
    "### Transformer Model\n",
    "\n",
    "In this subsection, we load the trained Transformer model and generate its predictions on the prepared labeled dataset.\n",
    "\n",
    "#### Model overview\n",
    "\n",
    "The Transformer model uses self-attention mechanisms to capture dependencies across the entire input sequence, enabling parallel computation and effective long-range pattern recognition in time series data.\n",
    "\n",
    "#### Mathematical formulation\n",
    "\n",
    "The Scaled Dot-Product Attention mechanism in the Transformer is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ Q $ = Query matrix  \n",
    "- $ K $ = Key matrix  \n",
    "- $ V $ = Value matrix  \n",
    "- $ d_k $ = dimension of the key vectors\n",
    "\n",
    "---\n",
    "\n",
    "The Multi-Head Attention combines multiple attention heads:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "where each head is computed as:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Load model architecture with tuned dimensions, number of heads, and layers.  \n",
    "2. Load trained weights from the saved `.pth` file.  \n",
    "3. Prepare input tensors as sequential feature windows.  \n",
    "4. Generate predictions and extract class labels for ensemble integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c302cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Transformer: dim_model=64, num_heads=4, num_layers=1, lr=0.001, batch_size=64\n",
      "Epoch [1/30], Loss: 1.0001\n",
      "Epoch [10/30], Loss: 0.6452\n",
      "Epoch [20/30], Loss: 0.5743\n",
      "Epoch [30/30], Loss: 0.6010\n",
      "\n",
      "Training Transformer: dim_model=64, num_heads=4, num_layers=2, lr=0.001, batch_size=64\n",
      "Epoch [1/30], Loss: 1.0637\n",
      "Epoch [10/30], Loss: 0.7357\n",
      "Epoch [20/30], Loss: 0.5832\n",
      "Epoch [30/30], Loss: 0.5632\n",
      "\n",
      "Training Transformer: dim_model=64, num_heads=8, num_layers=1, lr=0.001, batch_size=64\n",
      "Epoch [1/30], Loss: 1.0220\n",
      "Epoch [10/30], Loss: 0.7138\n",
      "Epoch [20/30], Loss: 0.6098\n",
      "Epoch [30/30], Loss: 0.5601\n",
      "\n",
      "Training Transformer: dim_model=64, num_heads=8, num_layers=2, lr=0.001, batch_size=64\n",
      "Epoch [1/30], Loss: 1.1155\n",
      "Epoch [10/30], Loss: 0.7246\n",
      "Epoch [20/30], Loss: 0.5988\n",
      "Epoch [30/30], Loss: 0.6063\n",
      "\n",
      "Training Transformer: dim_model=128, num_heads=4, num_layers=1, lr=0.001, batch_size=64\n",
      "Epoch [1/30], Loss: 1.0381\n",
      "Epoch [10/30], Loss: 0.7545\n",
      "Epoch [20/30], Loss: 0.6222\n",
      "Epoch [30/30], Loss: 0.6186\n",
      "\n",
      "Training Transformer: dim_model=128, num_heads=4, num_layers=2, lr=0.001, batch_size=64\n",
      "Epoch [1/30], Loss: 1.1563\n",
      "Epoch [10/30], Loss: 0.7077\n",
      "Epoch [20/30], Loss: 0.6628\n",
      "Epoch [30/30], Loss: 0.6036\n",
      "\n",
      "Training Transformer: dim_model=128, num_heads=8, num_layers=1, lr=0.001, batch_size=64\n",
      "Epoch [1/30], Loss: 1.0185\n",
      "Epoch [10/30], Loss: 0.7778\n",
      "Epoch [20/30], Loss: 0.6525\n",
      "Epoch [30/30], Loss: 0.6426\n",
      "\n",
      "Training Transformer: dim_model=128, num_heads=8, num_layers=2, lr=0.001, batch_size=64\n",
      "Epoch [1/30], Loss: 1.1107\n",
      "Epoch [10/30], Loss: 0.7596\n",
      "Epoch [20/30], Loss: 0.5955\n",
      "Epoch [30/30], Loss: 0.5813\n",
      "\n",
      "=== Transformer Tuning Results ===\n",
      " dim_model  num_heads  num_layers  learning_rate  batch_size  total_return  annualized_return  sharpe_ratio  max_drawdown  num_trades\n",
      "        64          8           2          0.001          64      0.825368           0.098195      1.306074     -0.053994         161\n",
      "       128          4           2          0.001          64      1.001593           0.114063      1.282562     -0.065611         170\n",
      "        64          4           2          0.001          64      0.738389           0.089881      1.121331     -0.066108         159\n",
      "        64          8           1          0.001          64      0.730456           0.089106      1.065239     -0.066584         167\n",
      "       128          4           1          0.001          64      0.649439           0.081008      1.035620     -0.072203         153\n",
      "        64          4           1          0.001          64      0.614211           0.077381      0.892961     -0.074684         162\n",
      "       128          8           2          0.001          64      0.593946           0.075265      0.889963     -0.077123         172\n",
      "       128          8           1          0.001          64      0.587884           0.074627      0.862761     -0.082910         175\n",
      "Epoch [1/200], Loss: 1.1728\n",
      "Epoch [20/200], Loss: 0.5902\n",
      "Epoch [40/200], Loss: 0.5626\n",
      "Epoch [60/200], Loss: 0.5702\n",
      "Epoch [80/200], Loss: 0.5393\n",
      "Epoch [100/200], Loss: 0.5330\n",
      "Epoch [120/200], Loss: 0.4884\n",
      "Epoch [140/200], Loss: 0.4867\n",
      "Epoch [160/200], Loss: 0.4646\n",
      "Epoch [180/200], Loss: 0.4575\n",
      "Epoch [200/200], Loss: 0.4151\n",
      "\n",
      "Transformer retrained and saved as trained_transformer_best_config.pth\n"
     ]
    }
   ],
   "source": [
    "tune_transformer(ticker)\n",
    "best_transformer(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f558fab",
   "metadata": {},
   "source": [
    "### Ensemble Model Comparison and Evaluation\n",
    "\n",
    "In this section, we compare the performance of different *ensemble strategies* tested on our true out-of-sample dataset. Rather than evaluating each model individually, we focus exclusively on ensemble approaches that combine the predictive power of MLP, CNN, LSTM, and Transformer models.\n",
    "\n",
    "#### Ensemble Methods Evaluated\n",
    "\n",
    "1. Weighted Majority Voting Ensemble\n",
    "\n",
    "   Combines model predictions using fixed assigned weights to each model’s vote.\n",
    "\n",
    "   Formula:\n",
    "\n",
    "   $$\n",
    "   \\text{Final Decision} = \\arg \\max_{c} \\sum_{m=1}^{M} w_m \\cdot \\mathbb{1}_{\\{p_m = c\\}}\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $ w_m $ is the weight of model $ m $\n",
    "   - $ p_m $ is its predicted class\n",
    "   - $ \\mathbb{1}_{\\{p_m = c\\}} $ is an indicator if model $ m $ predicts class $ c $.\n",
    "\n",
    "2. Probabilistic Voting Ensemble (Unconstrained)\n",
    "\n",
    "   Averages the softmax probabilities across all models to derive the final prediction.\n",
    "\n",
    "   Formula:\n",
    "\n",
    "   $$\n",
    "   \\text{Final Decision} = \\arg \\max_{c} \\left( \\frac{1}{M} \\sum_{m=1}^{M} P_m(c) \\right)\n",
    "   $$\n",
    "\n",
    "   where $ P_m(c) $ is the predicted probability for class $ c $ from model $ m $.\n",
    "\n",
    "3. Probabilistic Voting Ensemble (Constrained)\n",
    "\n",
    "   Same as above, but executed with trading constraints such as:\n",
    "\n",
    "   - Maximum position size\n",
    "   - Stop-loss thresholds\n",
    "   - Take-profit thresholds\n",
    "\n",
    "   This reflects more realistic portfolio constraints.\n",
    "\n",
    "4. Probabilistic Voting Ensemble with Kelly Criterion Sizing\n",
    "\n",
    "   Uses averaged probabilistic predictions for signals, and dynamically sizes each trade based on the Kelly criterion calculated from historical win/loss ratios.\n",
    "\n",
    "   Formula:\n",
    "\n",
    "   $$\n",
    "   f^* = \\frac{bp - q}{b}\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $ f^* $: optimal fraction of capital to bet\n",
    "   - $ b $: odds received (average win / average loss)\n",
    "   - $ p $: probability of winning\n",
    "   - $ q = 1 - p $.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "\n",
    "For each ensemble method, we report:\n",
    "\n",
    "- Total Return\n",
    "- Annualized Return\n",
    "- Sharpe Ratio\n",
    "- Sortino Ratio\n",
    "- Maximum Drawdown\n",
    "- Win Rate\n",
    "- Profit Factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2eb46372",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "results_weighted_majority, portfolio_weighted_majority, trade_returns_weighted_majority, ensemble_predictions_weighted_majority, df_eval_weighted_majority = run_ensemble_backtest(ticker)\n",
    "results_kelly, portfolio_kelly, trade_returns_kelly, ensemble_predictions_kelly, df_eval_kelly = run_probabilistic_ensemble_backtest_with_kelly(ticker)\n",
    "results_prob_voting_uncon, portfolio_prob_voting_uncon, trade_returns_prob_voting_uncon, ensemble_predictions_prob_voting_uncon, df_eval_prob_voting_uncon = run_probabilistic_ensemble_backtest(ticker, position_size=1, stop_loss=1, take_profit=1)\n",
    "results_prob_voting_con_min_pos, portfolio_prob_voting_con_min_pos, trade_returns_prob_voting_con_min_pos, ensemble_predictions_prob_voting_con_min_pos, df_eval_prob_voting_con_min_pos = run_probabilistic_ensemble_backtest(ticker, position_size=1)\n",
    "results_prob_voting_tot_con, portfolio_prob_voting_tot_con, trade_returns_prob_voting_tot_con, ensemble_predictions_prob_voting_tot_con, df_eval_prob_voting_tot_con = run_probabilistic_ensemble_backtest(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c9d291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ensemble Model Comparison Summary ===\n",
      "                            Ensemble Method  Total Return  Annualized Return  Sharpe Ratio  Sortino Ratio  Max Drawdown  Win Rate  Profit Factor\n",
      "                   Weighted Majority Voting        2.5765             0.1659        0.5823         0.4820       -0.5828    0.6867         1.7498\n",
      "       Probabilistic Voting (Unconstrained)        1.1019             0.0936        0.4226         0.3604       -0.5484    0.6625         1.4621\n",
      "Probabilistic Voting (Constrained, Min Pos)        0.6302             0.0606        0.3472         0.2851       -0.5484    0.4493         1.1799\n",
      " Probabilistic Voting (Totally Constrained)        0.2870             0.0309        0.3500         0.2881       -0.1682    0.4493         1.1799\n",
      "        Probabilistic Voting + Kelly Sizing       -0.0225            -0.0027       -0.3128        -0.0287       -0.0348    0.3333         0.2599\n"
     ]
    }
   ],
   "source": [
    "metrics_weighted_majority = calculate_additional_metrics(portfolio_weighted_majority, trade_returns_weighted_majority)\n",
    "metrics_kelly = calculate_additional_metrics(portfolio_kelly, trade_returns_kelly)\n",
    "metrics_prob_voting_uncon = calculate_additional_metrics(portfolio_prob_voting_uncon, trade_returns_prob_voting_uncon)\n",
    "metrics_prob_voting_con_min_pos = calculate_additional_metrics(portfolio_prob_voting_con_min_pos, trade_returns_prob_voting_con_min_pos)\n",
    "metrics_prob_voting_tot_con = calculate_additional_metrics(portfolio_prob_voting_tot_con, trade_returns_prob_voting_tot_con)\n",
    "\n",
    "combined_metrics_weighted_majority = {**results_weighted_majority, **metrics_weighted_majority}\n",
    "combined_metrics_kelly = {**results_kelly, **metrics_kelly}\n",
    "combined_metrics_prob_voting_uncon = {**results_prob_voting_uncon, **metrics_prob_voting_uncon}\n",
    "combined_metrics_prob_voting_con_min_pos = {**results_prob_voting_con_min_pos, **metrics_prob_voting_con_min_pos}\n",
    "combined_metrics_prob_voting_tot_con = {**results_prob_voting_tot_con, **metrics_prob_voting_tot_con}\n",
    "\n",
    "table = ensemble_comparison_summary(combined_metrics_weighted_majority, combined_metrics_kelly, combined_metrics_prob_voting_uncon, combined_metrics_prob_voting_con_min_pos, combined_metrics_prob_voting_tot_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f253a5",
   "metadata": {},
   "source": [
    "### 3. Calmar Ratio\n",
    "\n",
    "Compares *annualized return to maximum drawdown* to assess return-risk tradeoff.\n",
    "\n",
    "$$\n",
    "\\text{Calmar Ratio} = \\frac{R_a}{|\\text{Max Drawdown}|}\n",
    "$$\n",
    "\n",
    "- $ R_a $: annualized return\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Omega Ratio\n",
    "\n",
    "Considers the *entire distribution of returns relative to a target threshold*.\n",
    "\n",
    "$$\n",
    "\\text{Omega Ratio} = \\frac{\\int_{r_T}^{\\infty} [1 - F(r)] \\, dr}{\\int_{-\\infty}^{r_T} F(r) \\, dr}\n",
    "$$\n",
    "\n",
    "- $ r_T $: target return (e.g. 0)  \n",
    "- $ F(r) $: cumulative distribution function of returns\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Gain to Pain Ratio\n",
    "\n",
    "Measures *total net gains relative to total absolute losses*.\n",
    "\n",
    "$$\n",
    "\\text{Gain to Pain} = \\frac{\\sum \\text{Returns}}{\\sum |\\text{Losses}|}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Profit Factor\n",
    "\n",
    "Calculates *gross profit divided by gross loss*.\n",
    "\n",
    "$$\n",
    "\\text{Profit Factor} = \\frac{\\text{Gross Profit}}{|\\text{Gross Loss}|}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Expectancy per Trade\n",
    "\n",
    "Indicates *average expected profit per trade* accounting for win/loss probabilities.\n",
    "\n",
    "$$\n",
    "\\text{Expectancy} = (\\text{Win Rate} \\times \\text{Avg Win}) - (\\text{Loss Rate} \\times \\text{Avg Loss})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 8. CAGR to Max Drawdown Ratio\n",
    "\n",
    "Compares *compound annual growth rate to maximum drawdown*.\n",
    "\n",
    "$$\n",
    "\\text{CAGR/MDD} = \\frac{\\text{CAGR}}{|\\text{Max Drawdown}|}\n",
    "$$\n",
    "\n",
    "- *CAGR* is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{CAGR} = \\left( \\frac{P_{end}}{P_{start}} \\right)^{\\frac{1}{n}} - 1\n",
    "$$\n",
    "\n",
    "where:\n",
    "  - $ P_{end} $: ending portfolio value  \n",
    "  - $ P_{start} $: starting portfolio value  \n",
    "  - $ n $: number of years"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
